"use strict";(self.webpackChunkmy_website_2=self.webpackChunkmy_website_2||[]).push([[4940],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>g});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=c(r),m=a,g=u["".concat(l,".").concat(m)]||u[m]||d[m]||o;return r?n.createElement(g,s(s({ref:t},p),{},{components:r})):n.createElement(g,s({ref:t},p))}));function g(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,s=new Array(o);s[0]=m;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[u]="string"==typeof e?e:a,s[1]=i;for(var c=2;c<o;c++)s[c]=r[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,r)}m.displayName="MDXCreateElement"},3409:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var n=r(7462),a=(r(7294),r(3905));const o={slug:"scraping-arcgis-rest-api",title:"Scraping ArcGIS REST API",authors:"ecolazy",tags:["hello","docusaurus"]},s="Title: A Guide to Web Scraping Data Using Python and Requests",i={permalink:"/blog/scraping-arcgis-rest-api",editUrl:"https://github.com/ecolazy/blog/2023-08-12-scraping-arcgis-rest-api.md",source:"@site/blog/2023-08-12-scraping-arcgis-rest-api.md",title:"Scraping ArcGIS REST API",description:"Web scraping has become an essential tool for collecting and analyzing data from various websites. In this blog post, we will explore how to scrape data using the Python requests library. We'll walk through a sample script that demonstrates how to extract information from web services and save it as GeoJSON files.",date:"2023-08-12T00:00:00.000Z",formattedDate:"August 12, 2023",tags:[{label:"hello",permalink:"/blog/tags/hello"},{label:"docusaurus",permalink:"/blog/tags/docusaurus"}],readingTime:2.255,hasTruncateMarker:!1,authors:[{name:"ecolazy",title:"Maintainer",url:"https://ecolazy.github.io",key:"ecolazy"}],frontMatter:{slug:"scraping-arcgis-rest-api",title:"Scraping ArcGIS REST API",authors:"ecolazy",tags:["hello","docusaurus"]},prevItem:{title:"bristol house price heatmap",permalink:"/blog/bristol-house-price-heatmap"},nextItem:{title:"mastermap on aws arm64",permalink:"/blog/mastermap-on-aws-arm64"}},l={authorsImageUrls:[void 0]},c=[{value:"Web scraping has become an essential tool for collecting and analyzing data from various websites. In this blog post, we will explore how to scrape data using the Python requests library. We&#39;ll walk through a sample script that demonstrates how to extract information from web services and save it as GeoJSON files.",id:"web-scraping-has-become-an-essential-tool-for-collecting-and-analyzing-data-from-various-websites-in-this-blog-post-we-will-explore-how-to-scrape-data-using-the-python-requests-library-well-walk-through-a-sample-script-that-demonstrates-how-to-extract-information-from-web-services-and-save-it-as-geojson-files",level:2},{value:"Introduction to Web Scraping",id:"introduction-to-web-scraping",level:2},{value:"Setting Up the Environment",id:"setting-up-the-environment",level:2},{value:"Explanation of the Script",id:"explanation-of-the-script",level:2},{value:"Conclusion",id:"conclusion",level:2}],p={toc:c};function u(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"web-scraping-has-become-an-essential-tool-for-collecting-and-analyzing-data-from-various-websites-in-this-blog-post-we-will-explore-how-to-scrape-data-using-the-python-requests-library-well-walk-through-a-sample-script-that-demonstrates-how-to-extract-information-from-web-services-and-save-it-as-geojson-files"},"Web scraping has become an essential tool for collecting and analyzing data from various websites. In this blog post, we will explore how to scrape data using the Python requests library. We'll walk through a sample script that demonstrates how to extract information from web services and save it as GeoJSON files."),(0,a.kt)("h2",{id:"introduction-to-web-scraping"},"Introduction to Web Scraping"),(0,a.kt)("p",null,"Web scraping involves programmatically extracting data from websites. It can be used for a variety of purposes, including data analysis, research, and automation. However, it's crucial to understand the legal and ethical considerations surrounding web scraping, including respect for website terms of use and applicable laws."),(0,a.kt)("h2",{id:"setting-up-the-environment"},"Setting Up the Environment"),(0,a.kt)("p",null,"Before we dive into the code, make sure you have the requests library installed. You can install it using the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"pip3 install requests\n")),(0,a.kt)("p",null,"The Script: Scraping GeoJSON Data\nLet's consider a scenario where we want to scrape GeoJSON data from a web service. Here's a sample script that demonstrates the process:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'python\nCopy code\nimport requests\n\nbase_url = "https://example.com/server/rest/services/ext/"\noutput_folder = "output/"\n\nlist_items = [\n    "item_1", "item_2"\n]\n\nfor list_item in list_items:\n    for endpoint in range(0, 100):\n        query_url = f"{base_url}{list_item}/FeatureServer/{endpoint}/query?where=1%3D1&outFields=*&returnGeometry=true&f=geojson"\n\n        try:\n            response = requests.get(query_url, timeout=1)  # Corrected the placement of timeout parameter\n            response.raise_for_status()  # Check for HTTP errors\n\n            # Save response content as GeoJSON\n            geojson_filename = f"{output_folder}{list_item}_{endpoint}.geojson"\n            with open(geojson_filename, "wb") as f:\n                f.write(response.content)\n\n            print(f"Saved GeoJSON for endpoint {endpoint} of {list_item} to {geojson_filename}")\n\n        except requests.exceptions.RequestException as e:\n            print(f"Error retrieving data for endpoint {endpoint} of {list_item}: {e}")\n\n')),(0,a.kt)("h2",{id:"explanation-of-the-script"},"Explanation of the Script"),(0,a.kt)("p",null,"Import the requests library to make HTTP requests."),(0,a.kt)("p",null,"Define the base_url and output_folder for the web service and output files, respectively."),(0,a.kt)("p",null,"Create a list of list_items representing the items you want to scrape."),(0,a.kt)("p",null,"Use nested loops to iterate through each list_item and a range of endpoints."),(0,a.kt)("p",null,"Construct the query URL by combining base_url, list_item, and the endpoint."),(0,a.kt)("p",null,"Send an HTTP GET request using requests.get() to fetch data from the URL."),(0,a.kt)("p",null,"Check for HTTP errors using response.raise_for_status()."),(0,a.kt)("p",null,"Save the GeoJSON response content as a file in the specified output folder."),(0,a.kt)("p",null,"Handle exceptions using a try-except block to catch request-related errors."),(0,a.kt)("p",null,"Print messages indicating the progress and status of the scraping process."),(0,a.kt)("h2",{id:"conclusion"},"Conclusion"),(0,a.kt)("p",null,"Web scraping with Python and the requests library provides a powerful way to extract and collect data from various web services. However, it's essential to follow ethical guidelines, respect website terms of use, and be aware of legal considerations. The provided sample script serves as a starting point for scraping GeoJSON data, and you can adapt it for other data types and websites."),(0,a.kt)("p",null,"Remember to always review the website's terms of use and consider the ethical implications of scraping before proceeding. Happy scraping!"))}u.isMDXComponent=!0}}]);