---
slug: constructing-a-3d-egg-shape-from-regular-polygons
title: Constructing a 3D Egg Shape from Regular Polygons 
authors: ecolazy
tags: [hello, docusaurus]
---




# Constructing a 3D Egg Shape from Regular Polygons 

As engineers and designers, we are often approached with unusual and challenging requests. One such request came from an artist who asked us to help find a suitable geometry for a large egg sculpture. Intrigued by the opportunity to collaborate with an artist and apply our skills in a creative context, we set out to identify possible geometries using generative algorithms in Grasshopper 3D.

The first step in our process was to understand the constraints and requirements of the project. The artist provided us with a set of parameters, including the desired size and shape of the sculpture, as well as the materials that would be used to construct it. With this information in hand, we began to explore different algorithmic approaches that could be used to generate a range of possible geometries.

We turned to Grasshopper 3D, a powerful tool for generative design that allows users to define and manipulate geometry through a series of algorithmic rules. Using a combination of mathematical functions and input from the artist, we were able to generate a range of potential geometries for the egg sculpture.

To narrow down our options and select the most suitable geometry, we used a variety of methods to evaluate and compare the different options. We considered factors such as structural integrity, ease of fabrication, and aesthetic appeal, and used computer simulations and physical prototypes to test the performance of the different geometries.

In the end, we were able to identify a geometry that met all of the requirements and constraints of the project. The artist was pleased with the result and used our geometry to create a stunning egg sculpture that was well-received by the public.

This project was a rewarding and exciting example of the ways in which engineering and design can intersect with the arts. By using generative algorithms and a collaborative approach, we were able to help an artist realize their vision and create a beautiful work of art.


![Docusaurus Plushie](/img/egg-1.jpg)

![Docusaurus Plushie](/img/egg-2.jpg)---
slug: designing-hardware-for-mounting-wireless-hardware-equiptment-on-marquees
title:  Designing Hardware for Mounting Wireless Equipment on Marquees
authors: ecolazy
tags: [hola, docusaurus]
---

# Designing Hardware for Mounting Wireless Equipment on Marquees

Wireless access points and point-to-point links have been installed on the aluminum extrusion structural elements of outdoor event marquees using a innovative system that employs a keeder rail. The keeder rail accommodates a cylindrical metal piece with a threaded hole drilled perpendicular to its length, which can be easily secured in place by turning a large, flat disk with a threaded hole in the center. This setup supports a pipe clamp that is designed to fit standard aluminum poles, with two clamps used per pole to securely anchor the pole vertically on the side of the tent. This innovative method of installation allows for reliable and secure connectivity during events, while also maintaining the aesthetic integrity of the marquees.


![Docusaurus Plushie](/img/clamp-2.jpg)


![Docusaurus Plushie](/img/clamp-3.jpg)
---
slug: wireless-coverage-at-manchester-pride
title: Wireless Coverage at Manchester Pride - A Heatmap Analysis
authors: ecolazy
tags: [hola, docusaurus]
---

# Wireless Coverage at Manchester Pride - A Heatmap Analysis

As part of this deployment project, our team encountered a physically demanding challenge: the installation of two sectors on the 18th floor of a tower block adjacent to the event site. From this vantage point, we were able to transmit a strong signal to multiple wireless point-to-point stations attached to temporary structures and buildings within the site. Additionally, we provided indoor wireless coverage in an underground parking garage that served as one of the event venues. To ensure optimal coverage, we created a heat map of the wireless signal strength in this area. Moreover, we established uplinks at various locations throughout the site to support ticket booths, CCTV cameras, payment terminals for bars, a production office, and emergency liaison cabins. Overall, this comprehensive approach to wireless connectivity allowed for seamless communication and connectivity throughout the event.---
slug: exploring-the-unique-characteristics-of-natural-building-materials
title: Exploring the Unique Characteristics of Natural Building Materials
authors: ecolazy
tags: [hola, docusaurus]
---

# Exploring the Unique Characteristics of Natural Building Materials

Inspired by the idea of using natural building materials, we set out to explore the unique characteristics and personalities of each material and use them in innovative ways in our design. We wanted to capture the mood and atmosphere of the natural environments where these materials are typically found, such as the underground world of stone and water or the airy heights of wood.

To this end, we experimented with lightweight hazel structures as a way to combine wood and air in our design. We were particularly interested in the underground world of stone and water, and decided to dig a tunnel into the side of a hill to see what we could discover. Unfortunately, the largest cavity we found was only two feet wide, and we had to come up with a different solution to support the roof of the tunnel. We ended up building concrete block load-bearing walls with concrete lintels supporting the stone above, which took up a lot of space and lost some of the rustic charm of our original design, but was necessary for safety.

According to our cross-sectional diagram, there is a third layer between the stone and wood in our design - a thin layer of mud from which all life springs. While we have not yet conducted extensive research on using mud as a building material, we have done extensive research and design work on the organic life that it can support. In our design, we envision the wood as the ideal location for sleeping quarters, warm and dry in the loft, while the ground floor would be reserved for the kitchen, toilet, and other organic activities, including food production. There may also be some kind of underground stone cavernous space beneath the building for an as-yet-unknown purpose.

Overall, our goal in this project was to use natural building materials in unique and innovative ways that captured the mood and atmosphere of the natural environments where these materials are typically found. While we encountered some challenges along the way, the process was exciting and rewarding, and we are proud of the design we have created.---
slug: vertical-farming-for-continuous-salad-production
title: Vertical Farming for Continuous Salad Production
authors: ecolazy
tags: [hola, docusaurus]
---

# Vertical Farming for Continuous Salad Production

Our team has an innovative idea for a self-sustaining salad bar that utilizes vertical space to meet production needs. We propose planting seedlings in a steel cable reinforced fiber matting that hangs vertically above the counter. This matting would be supported by two large drums at the top and bottom, and would be able to rotate at a speed calculated based on the height of the assembly, the maturation time of the chosen crop, and the desired harvesting frequency. This way, a seedling planted at the bottom would be ready for harvest by the time it reaches the top and returns to the bottom. With this system, the salad bar would be able to achieve perpetual harvesting, meeting its demand for fresh produce without having to rely on traditional terrestrial agriculture methods.

In addition to this innovative planting system, we also propose supplying the plants with dissolved nutrients by producing an ultrasonic fog in the chamber between the two planting surfaces. This would help to ensure that the plants have all the nutrients they need to grow and thrive. To maintain a stable temperature for the plants, we suggest using a vertical poly tunnel in colder climates to protect them from temperature fluctuations.

Overall, our self-sustaining salad bar concept would be a revolutionary way to meet the demand for fresh produce in a sustainable and efficient manner. By utilizing vertical space and innovative technologies, we believe this concept has the potential to revolutionize the way that salad bars and other food establishments source their produce.---
slug: designing-a-living-building:-an-organism-with-a-symbiotic-relationship-with-its-occupants
title: Designing a Living Building - An Organism with a Symbiotic Relationship with Its Occupants
authors: ecolazy
tags: [hola, docusaurus]
---

# Designing a Living Building: An Organism with a Symbiotic Relationship with Its Occupants

When designing a building, it is important to consider the building as an organism that has a symbiotic relationship with its occupants. This means that the building should be organized like a system of interdependent organisms, with each organism fulfilling a specific function within the larger structure. By treating the building as an organism, we can create a dynamic and self-sustaining ecosystem that supports the health and well-being of its occupants.

There are many different types of organisms that can be incorporated into the design of a building. Some examples might include microalgae, which can be used for wastewater treatment and oxygen production; vermiculture, which can be used for composting and soil improvement; fish, which can be used for food production and waste management; guinea pigs, which can be used for research and education; leafy green plants, which can be used for air purification and aesthetics; and bacteria, which can be used for biological processes such as fermentation and nutrient cycling.

The building itself should function like a shell or frame that provides a home for these organisms. By using environment-specific creatures, it is possible to keep different systems separate, creating an urban jungle inside the building. This approach allows for the creation of a diverse and self-sustaining ecosystem within the building, providing numerous benefits to the occupants such as improved air quality, food production, and waste management. By considering the building as an organism with a symbiotic relationship with its occupants, we can design buildings that are more dynamic, self-sustaining, and supportive of human health and well-being.---
slug: designing-for-comfort-and-practicality-in-motor-caravans
title: Designing for Comfort and Practicality in Motor Caravans
authors: ecolazy
tags: [hello, docusaurus]
---

# Designing for Comfort and Practicality in Motor Caravans
As engineers and designers, it is our duty to create structures that not only function well, but also enhance the lives of those who use them. This is particularly important in the design of motor caravans, which serve as both vehicles and living spaces for travelers. With this in mind, we set out to design a caravan that prioritized both comfort and practicality.

The first step in our process was to choose the most suitable materials and construction techniques. We decided to use a thick layer of spray foam as the foundation for the caravan's shape, which was then carved back to the desired form and rendered with acrylic putty. This method allowed us to create a strong and lightweight structure, while also providing excellent insulation to keep the interior comfortable.

Next, we focused on the layout of the interior space. We installed a single bed lengthways, with a desk running parallel to it. This arrangement allowed for maximum use of the limited space, while also providing a comfortable and functional sleeping and working area. In addition, we designed the layout in such a way that the user can easily walk from the back of the caravan to the side door unhindered, which gives flexibility in terms of access and egress.

To ensure practicality, we also included a bulkhead kitchen with a water-saving faucet, a full-size fridge, and a hob. These features allow the user to easily prepare meals and store food while on the road. We also installed double-layered Ikea blackout blinds and windows all around to provide privacy and regulate light and temperature within the caravan.

Overall, our design for this motor caravan prioritizes both comfort and practicality, making it an ideal living space for travelers. By carefully considering the materials, layout, and features, we have created a functional and enjoyable space that will enhance the experience of those who use it.

![Docusaurus Plushie](/img/ergo-1.png)
![Docusaurus Plushie](/img/ergo-2.png)
![Docusaurus Plushie](/img/ergo-3.png)
![Docusaurus Plushie](/img/ergo-4.png)
![Docusaurus Plushie](/img/ergo-5.png)
![Docusaurus Plushie](/img/ergo-6.png)
![Docusaurus Plushie](/img/ergo-7.png)
![Docusaurus Plushie](/img/ergo-8.png)
![Docusaurus Plushie](/img/ergo-9.png)
![Docusaurus Plushie](/img/ergo-10.png)
![Docusaurus Plushie](/img/ergo-11.png)
![Docusaurus Plushie](/img/ergo-12.png)
![Docusaurus Plushie](/img/ergo-13.png)---
slug: wireless-coverage-at-the-bath-festival-a-heatmap-analasis
title: Wireless Coverage at the Bath Festival - A Heatmap Analysis
authors: ecolazy
tags: [hola, docusaurus]
---

# Wireless Coverage at the Bath Festival - A Heatmap Analysis

At the Bath Festival, a small deployment of access points was placed around the tents to provide WiFi connectivity to attendees. In order to maximize coverage and ensure reliable connectivity, each access point was connected to a sector on an adjacent building via a point-to-point wireless link. In addition to the wireless link, each access point was also connected to a ADSL line and a temporary satellite on the roof to provide multiple redundant Internet connections.

To assess the performance of the access points and identify any areas with weak signal strength or wireless black spots, we used an Android app to collect a series of geolocated signal strength data points. These data points were formatted in Excel and then uploaded into ArcGIS, where a tool was used to create a heat map. This heat map was then overlayed over the site plan to provide a visual representation of the signal strength across the festival grounds.

By collecting and analyzing this data, we were able to identify any areas with weak signal strength or wireless black spots and take steps to improve coverage in these areas. This ensured that attendees were able to access the WiFi network and stay connected throughout the duration of the festival. By using a combination of a point-to-point wireless link, a ADSL line, and a temporary satellite, we were able to provide reliable and redundant Internet connectivity to the festival attendees.---
slug: using-data-queries-to-enhance-wildlife-connectivity
title: Using Data Queries to Enhance Wildlife Connectivity
authors: ecolazy
tags: [hello, docusaurus]
---

# Using Data Queries to Enhance Wildlife Connectivity

Rewilding involves the restoration of natural habitats in areas that have been modified or degraded by human activity. One way to identify suitable areas for rewilding is to search for topographic areas that are characterized by natural environments. This can be achieved through the use of a SQL query to search for rows in a database table where the value "Natural Environment" appears in the "descriptivegroup" column. This column likely contains an array of descriptive tags or categories for each topographic area.

The results of this query can inform planning and conservation efforts by identifying areas that are potentially well-suited for rewilding. These areas could provide corridors for wildlife movement through urban environments and enhance biodiversity in these areas. By prioritizing these areas for restoration and rewilding, it is possible to improve the connectivity of natural habitats in urban areas and promote the health and well-being of these ecosystems.

``` sql
SELECT *
FROM topographicarea
WHERE 'Natural Environment' = ANY (descriptivegroup)
```

![Docusaurus Plushie](/img/wildlife-corridors-2.png)

![Docusaurus Plushie](/img/wildlife-corridors-2.png)

You can see how the edges or roads and railway tracs could be used as wildlife corridors.

![Docusaurus Plushie](/img/wildlife-corridors-3.png)
---
slug: live-network-map-for-womad-festival
title: Live Network Map for WOMAD Festival - Visualizing Status and Coverage
authors: ecolazy
tags: [hello, docusaurus]
---

# Live Network Map for WOMAD Festival - Visualizing Status and Coverage

The web application we developed allows users to access and update the location data for network devices. When the location of a device needs to be recorded, the user simply enters the device's MAC address into the app. The MAC address is then checked against a list of available device MAC addresses in the database to verify its authenticity. If the MAC address exists in the database, it is marked as "deployed" and the coordinates of the user's phone, on which the update was made, are added to the latitude and longitude columns. If the MAC address is entered incorrectly or does not correspond to a device in the database, the app user is notified and asked to enter a different MAC address.

The deployed devices are displayed on a map in real-time, allowing users to easily view and locate them. Each device can be clicked on to view information such as its device type, MAC address, IP address, and more. Users also have the option to select a device for deletion, which changes the corresponding value in the "deployment status" column to "false" and removes the latitude and longitude position values from the database.

To aid in testing and debugging the app, we also developed a BASH script that produces fake data for testing purposes. This script generates a CSV file containing random MAC addresses, asset tags, device models, and locations, which can then be uploaded to the database for testing purposes. By using this script, we were able to simulate different scenarios and ensure that the app was functioning correctly before deploying it in a live environment.

``` bash
#!/bin/bash

# Generate 100 random devices
for i in {1..100}
do
  # Generate a random MAC address
  mac=$(c=0; until [ $c -eq "6" ]; do printf ":%02X" $(( $RANDOM % 256 )); let c=c+1; done | sed s/://)

  # Generate a random asset number
  asset=$(( $RANDOM % 9999 + 1000 ))

  # Choose a random location from the locations.txt file
  location=$(shuf -n 1 locations.txt)

  # Choose a random model from the models.txt file
  model=$(shuf -n 1 models.txt)

  # Output the device information to a CSV file
  echo "$asset, $mac, $model, $location"
done > devices.csv
```


---
slug: mapping-wireless-coverage-at-the-royal-windsor-horse-show-with-arcgis
title: Mapping Wireless Coverage at the Royal Windsor Horse Show with ArcGIS
authors: ecolazy
tags: [hola, docusaurus]
---

# Mapping Wireless Coverage at the Royal Windsor Horse Show with ArcGIS



The Royal Windsor Horse Show is an annual event that attracts thousands of visitors and is the largest outdoor horse show in the United Kingdom. In order to ensure that the event runs smoothly, it is important to provide reliable connectivity to the staff and trader areas.

To achieve this, our team deployed a wireless network on the show grounds and created a heat map by taking geolocated signal strength readings on a smartphone from all over the site. This data was then uploaded into ArcGIS and used to create a heat map, which highlighted areas with weak signal strength or wireless black spots.

To help visualize the network and identify any potential issues, the heat map was overlaid on a georeferenced site map that showed the locations of wireless access points, cable routes, and network switch locations. By using this information, we were able to ensure that the staff and trader areas had the necessary connectivity to support the needs of the event.

Overall, the deployment of the wireless network at the Royal Windsor Horse Show was a success and played a vital role in the smooth running of the event. We are proud to have been able to support the needs of the staff and traders and help make the event a success.

![Docusaurus Plushie](/img/windsor-2.png)

---
slug: tracking-network-hardware-assets-at-the-isle-of-wight-festival
title: Tracking Network Hardware Assets at the Isle of Wight Festival
authors: ecolazy
tags: [hello, docusaurus]
---

# Tracking Network Hardware Assets at the Isle of Wight Festival

The Isle of Wight Festival 2019 was a large-scale event that took place at Seaclose Park on the Isle of Wight. With 3km of arenas and fields to cover, the deployment of the network was a massive and complex undertaking. To ensure that the site was properly connected, network cabinets were set up in each arena and connected with miles of fiber optic cable. These cabinets were then connected to multiple ADSL connections that were bonded together, providing a robust and reliable internet connection.

To provide connectivity within each arena, smaller network switches were located within 100m of the cabinets. Locations that were beyond this range were connected with wireless point-to-point links, which were either mounted on poles attached to the sides of tents or beam out across the arena from wireless sectors mounted on cherry pickers. These signals were then picked up at distant locations through wireless point-to-point receivers attached to the sides of tents.

In addition to the network infrastructure, the event also required a comprehensive CCTV system to ensure the safety of attendees. To this end, at least one cherry picker equipped with a pan-tilt-zoom CCTV camera was stationed within each arena, with additional cameras installed on scaffolding poles, gateway arches, and stage sides. WiFi was also provided in the crew and camping areas, and temporary offices were equipped with temporary WiFi and VoIP phones for both internal and external communication.

To aid in the deployment and management of the network, we utilized a number of tools and resources. For example, we used QGIS's 'Align Raster' tool to georeference a high-definition image of the site map, which we then uploaded to Mapbox and used to create a basic Leaflet.js web map. This map used the host phone's geolocation to position a marker, helping us to determine our exact location on the site and identify which tents required connectivity. We also used the 'Map Marker' app on Android to quickly locate network devices as we deployed them.

After the event, the map was used to quickly locate and retrieve all of the equipment. This was particularly useful as the staff members who investigate faults or retrieve hardware after an event are often different from those who deployed it, making it difficult to locate the devices without a detailed and up-to-date map showing their locations and connections. By using this map, we were able to efficiently trace faults in the network and ensure that all of the equipment was properly accounted for. Overall, the deployment and management of the network at the Isle of Wight Festival was a successful and complex endeavor that helped to ensure the smooth operation of the event.

---
slug: provisioning-cisco-cloud-wireless-controller
title: Provisioning Cisco Cloud Wireless Controller
authors: ecolazy
tags: [hola, docusaurus]
---

# Provisioning Cisco Cloud Wireless Controller

In this project, we aimed to successfully install and configure the Cisco Catalyst c9800-CL wireless controller using KVM (Kernel-based Virtual Machine). The c9800-CL is a powerful and flexible cloud-based wireless controller that is capable of managing both on-premises and cloud-based wireless networks. It belongs to the Cisco Catalyst 9800 series and offers a range of advanced features such as wireless intrusion prevention, location services, and guest access.

To begin, we installed virtualization software and enabled the libvirtd service on our system. This allowed us to create and manage virtual machines using KVM. We then created a network bridge using the brctl command, which enabled communication between the virtual machine and the host system.

With the necessary infrastructure in place, we used the virt-install command to install the c9800-CL on a new virtual machine. During the installation process, we specified a number of options such as the connection to the virtualization server, the operating system variant, the architecture of the virtual machine, and the CPU type.

Once the virtual machine was set up and the c9800-CL was installed, we provided a script to configure the controller. This script contained a series of steps that were necessary to properly set up the c9800-CL. These steps included setting the hostname, creating a user account, configuring the Gigabit Ethernet interfaces, creating a VLAN, setting up static routes, shutting down and re-enabling the radio frequencies, and setting the country code. We also configured the virtual wireless LAN controller (VWLC) and set the DNS and NTP servers to ensure proper network connectivity and synchronization.

Finally, we demonstrated how to access the GUI of the c9800-CL at the specified IP address and walk through the zero-day configuration steps to set up a wireless network. By following these steps, users can easily configure the c9800-CL to meet the specific needs of their wireless network.---
slug: creating-3d-buildings-from-mastermap-with-qgis
title: Creating 3d Buildings From Mastermap With Qgis
authors: ecolazy
tags: [hola, docusaurus]
---

# Creating 3d Buildings From Mastermap With Qgis

The Ordnance Survey Mastermap Topography Layer, the Building Height Attribute (BHA), and the Environment Agency LiDAR Digital Terrain Model (DTM) are all useful data sources that can be used to create 3D models of buildings. By combining this data and using the Qgis2ThreeJS plugin in QGIS, it is possible to visualize the BHA data in 3D and create a 3D model of a building.

To do this, the Qgis2ThreeJS plugin must be installed and the BHA data, DTM data, and any additional desired layers must be loaded into the QGIS project. The plugin can then be used to style the BHA data and specify the desired height attribute for extrusion, resulting in a 3D model of the building. This model can be saved as an HTML file and viewed in a web browser.

When combined with the LiDAR DTM, the resulting 3D model is fairly accurate and can be opened in Grasshopper, a visual programming language and environment that runs within the Rhinoceros 3D CAD application. With the addition of the Ladybug plugin, this 3D model can be used to perform detailed analyses of climate data and create customized, interactive visualizations for environmentally-informed design, such as sunlight studies.

![Docusaurus Plushie](/img/bha-1.png)
![Docusaurus Plushie](/img/bha-2.png)
![Docusaurus Plushie](/img/bha-3.png)
![Docusaurus Plushie](/img/bha-4.png)
![Docusaurus Plushie](/img/bha-5.png)
![Docusaurus Plushie](/img/bha-6.png)
![Docusaurus Plushie](/img/bha-7.png)
![Docusaurus Plushie](/img/bha-8.png)
![Docusaurus Plushie](/img/bha-9.png)
![Docusaurus Plushie](/img/bha-10.png)
![Docusaurus Plushie](/img/bha-11.png)
![Docusaurus Plushie](/img/bha-12.png)


# References
[qgis bha](https://digimap.edina.ac.uk/help/gis/qgis/qgis_bha/)

---
slug: gifford-circus-cry-wheel-performances
title: Gifford Circus Cyr Wheel Performances - A Custom Modular Circular Stage
authors: ecolazy
tags: [hola, docusaurus]
---
![Docusaurus Plushie](/img/stage-3.jpg)
![Docusaurus Plushie](/img/stage-4.jpg)


# Gifford Circus Cyr Wheel Performances - A Custom Modular Circular Stage
We were commissioned to design and construct a circular stage for a Cyr wheel performance in Gifford's Circus. The stage had to meet several requirements: it had to have a diameter of 6 meters, be able to be disassembled into pieces small enough to fit through a door with a width of 2.6 meters, be able to support the weight of performers, be easy to assemble by two people in a short amount of time (with each panel weighing no more than 50kg), be flat and level on rough ground, and be durable enough to withstand the rough handling and moisture of circus life.

To achieve these goals, we considered various designs for the circle and ultimately settled on a composite panel made of an XPS core sandwiched between two layers of extra durable fibreglass, with a non-slip coating. The material was cut into the necessary shapes using a hot wire cutter with a rectangular profile, and the foam was removed from the mating edges, leaving only the outer fibreglass material. Adhesive was applied and aluminum extrusion was pressed into the edges of the panels. The aluminum provided protection for the edges, supported the edges to prevent deflection, and provided slots for the removable leg structures that connected the panels together.

The leg structures were placed at the intersection of three panels and consisted of three pieces of steel box section fitted within aluminum U channels and welded at 120-degree increments to form a three-directional cross. A hole was left at the intersection, into which a threaded tube was welded. An internal hex bolt or long grub screw with a foot plate on the bottom was threaded into the tube and could be adjusted from above through a small hole in the stage using an allen key bit on an impact driver. This arrangement allowed for a quick and easy assembly process, with the legs being extended until they touched the ground to level the stage. The pieces were held together with inner steel box sections curved on the outer curved edges, and a large ratchet strap was used to wrap around the entire structure and compress the circle inward, pulling all the pieces together.

Overall, the design and construction of the circular stage was a challenging but rewarding project. We were able to meet all of the requirements set forth by the client and create a stage that was durable, stable, and easy to assemble and transport. 

![Docusaurus Plushie](/img/stage-7.jpg)

![Docusaurus Plushie](/img/stage-8.jpg)
![Docusaurus Plushie](/img/stage-2.jpg)

![Docusaurus Plushie](/img/stage-6.jpg)

![Docusaurus Plushie](/img/stage-1.jpg)

![Docusaurus Plushie](/img/stage-10.jpg)
---
slug: truss-connection-node-modeling
title:  Truss Connection Node Modeling
authors: ecolazy
tags: [hola, docusaurus]
---

# Truss Connection Node Modeling

![Docusaurus Plushie](/img/truss-3.png)


In this project, we designed and fabricated a dodecahedron-shaped structural node for use in a truss system. A truss is a structural element that consists of a series of interconnected struts, which work together to distribute loads evenly and maintain the stability and strength of the structure. The node we created had 12 pentagonal faces, and each truss strut was attached to the center of one of these faces via a single bolt. This bolt passed through a hole in the center of the end cap of the strut and was secured in place by screwing it into a threaded hole in the center of the dodecahedron face.

The node was made of steel and was designed to resist the forces transmitted through the truss. Its dodecahedron shape and the use of a single bolt per strut allowed for a high level of flexibility and adaptability, as the struts could be easily rearranged or removed due to their modular design. This feature made the node a crucial element in the overall design of the truss, as it enabled the structure to be easily modified or altered to meet changing needs or requirements.

Overall, the dodecahedron-shaped structural node we created proved to be an effective and efficient solution for joining multiple truss struts together at a single point. It played a vital role in distributing loads evenly and maintaining the stability and strength of the truss, and its modular design allowed for flexibility and adaptability in the overall structure.


![Docusaurus Plushie](/img/truss-1.jpg)

![Docusaurus Plushie](/img/truss-2.jpg)
---
slug: modeling-a-spherical-solar-concentrator
title: Modeling a Spherical Solar Concentrator
authors: ecolazy
tags: [hello, docusaurus]
---

# Modeling a Spherical Solar Concentrator
![Docusaurus Plushie](/img/solar-concentrator-3.jpg)

If a mirror is positioned on the surface of a sphere and is perpendicular to a line that extends from the center of the sphere to a point on the surface of the sphere, light from outside the sphere that is directed towards the mirror will be reflected off the mirror's surface and into the center of the sphere.

![Docusaurus Plushie](/img/solar-concentrator-1.jpg)


![Docusaurus Plushie](/img/solar-concentrator-4.jpg)

## Background

Solar energy is a renewable energy source that has the potential to meet a significant portion of the world's energy needs. However, solar energy is currently not as widely used as other energy sources due to the difficulty of capturing and concentrating solar energy.


## Abstract

A solar concentrator is provided that uses a sphere with rotating mirrors to concentrate solar energy. The sphere is covered with a plurality of small mirrors, each of which can rotate freely in any direction. Light from outside the sphere that is directed towards any of the mirrors will be reflected off the mirror's surface and into the center of the sphere. The concentrated solar energy can then be used to generate electricity or heat water.

## Description

The solar concentrator comprises a sphere, a plurality of small mirrors, and a support structure. The sphere is covered with the plurality of small mirrors, each of which can rotate freely in any direction. The support structure supports the sphere and maintains it in a fixed position.

The small mirrors can be made of any material that is reflective of sunlight, such as glass, metal, or plastic. The support structure can be made of any material that is strong enough to support the weight of the sphere and that will not corrode in sunlight.

The solar concentrator can be used to generate electricity or heat water. To generate electricity, the concentrated solar energy can be used to heat a fluid, such as water or molten salt. The heated fluid can then be used to drive a turbine, which can generate electricity. To heat water, the concentrated solar energy can be used to heat water directly. The heated water can then be used for bathing, cooking, or other purposes.

## Advantages

The solar concentrator using a sphere with rotating mirrors has several advantages over other solar concentrators. First, the rotating mirrors are able to reflect sunlight from a wide range of angles, which makes it more efficient at capturing solar energy. Second, the rotating mirrors are able to concentrate solar energy into a small area, which makes it more efficient at generating electricity or heating water. Third, the rotating mirrors are relatively easy to manufacture and install, which makes it a cost-effective option for solar energy applications.

## Claims

A solar concentrator comprising:
a sphere;
a plurality of small mirrors; and
a support structure.
The sphere is covered with the plurality of small mirrors, each of which can rotate freely in any direction.
The support structure supports the sphere and maintains it in a fixed position.
The small mirrors are made of a material that is reflective of sunlight.
The support structure is made of a material that is strong enough to support the weight of the sphere and that will not corrode in sunlight.
The solar concentrator is used to generate electricity.
The solar concentrator is used to heat water.


## Conclusion

The solar concentrator using a sphere with rotating mirrors is a novel and efficient way to capture and concentrate solar energy. The concentrator has several advantages over other solar concentrators, including its ability to reflect sunlight from a wide range of angles, its ability to concentrate solar energy into a small area, and its relatively easy manufacture and installation. The concentrator has the potential to be a cost-effective option for solar energy applications.
---
slug: parish-house-prices
title: Parish House Prices - A Map of Average Sale Prices
authors: ecolazy
tags: [hello, docusaurus]
---

# Parish House Prices - A Map of Average Sale Prices

In order to accurately assess the real estate market, our team utilized property sale data from the land registry and postal code data from the Ordnance Survey to determine the latitude and longitude coordinates of each house sale. This data was then imported into a PostGIS database, where an SQL query was run to calculate the average home price for each parish. To facilitate the visualization and analysis of this information, we utilized the powerful mapping software QGIS. By coloring the polygons representing each parish based on the average price, we were able to clearly and intuitively display the variations in the housing market across the region.

This process allowed us to gain a detailed and nuanced understanding of the real estate market, and to identify trends and patterns that would not have been immediately apparent without the use of spatial analysis. By combining the robust data management capabilities of PostGIS with the intuitive mapping capabilities of QGIS, we were able to effectively and efficiently analyze complex data sets and extract valuable insights.


![Docusaurus Plushie](/img/price-paid-parish.jpg)---
slug: flood-risk-assessment-of-pudding-brook
title: Flood Risk Assessment of Pudding Brooke
authors: ecolazy
tags: [hola, docusaurus]
---

# Flood Risk Assessment of Pudding Brooke

In order to assess the flood risk of Pudding Brooke, we first imported Digital Terrain Model LIDAR tiles into QGIS and used them to create contour polygons at 0.25m intervals. These contour polygons were then styled using a graduated color scheme, with each polygon being colored based on its elevation. This allowed us to easily visualize the topography of the area and identify areas that were prone to flooding.

After creating the contour polygons, we selected potential sites for outbuildings in appropriate high ground locations. This was an important step in the flood risk assessment process, as we needed to ensure that any new structures that were built would be situated on high ground and therefore less vulnerable to flooding. To identify suitable locations for outbuildings, we carefully evaluated the contour polygons and selected sites that were located on the highest ground available.

Overall, the use of Digital Terrain Model LIDAR tiles and contour polygons was essential in helping us to accurately assess the flood risk of Pudding Brooke and identify suitable sites for outbuildings. By carefully analyzing the topography of the area, we were able to make informed decisions about where it was safe to build, ensuring that any new structures would be protected from potential flooding.

![Docusaurus Plushie](/img/pudding-brook.jpg)
---
slug: inspire-index-polygon-house-prices
title: INSPIRE Index Polygon House Prices - A Map of Average Sale Prices
authors: ecolazy
tags: [hello, docusaurus]
---

# INSPIRE Index Polygon House Prices - A Map of Average Sale Prices

## Prepare prices data

In order to effectively analyze and visualize real estate data, it is important to first properly organize and process the data. To this end, our team combined three separate files containing price paid data into a single file and cleaned and filtered the data through a series of steps. These steps included the removal of quotes, the selection of only rows with "GL" followed by a number, the printing of certain columns, the addition of column names, and the deletion of rows with null values.

Once the data was cleaned and organized, we used the powerful tool ogr2ogr to convert a file with cadastral parcel information into a PostgreSQL file. We then changed the projection from OSGB to WGS84 and imported it into a database. In order to store the data in a structured manner, we started a psql session and created empty tables with certain columns in the database.

Next, we used the \copy command and SQL JOIN to combine the price and coordinates data based on their shared postcodes. We added a column for geometry data and used the latitude and longitude data to create points. We then calculated the average value for each of the duplicate polygons.

Finally, we used the powerful mapping software QGIS to export the table from the database and modified the layer properties for visual appeal. Through this process, we were able to effectively organize and analyze the real estate data, allowing us to extract valuable insights and gain a deeper understanding of the market.


![Docusaurus Plushie](/img/price-paid-parcel.jpg)

During our analysis of real estate data, we encountered an issue with some of the postal codes not being properly associated with the intended polygons. This issue had the potential to significantly impact the accuracy and usefulness of our data.

To address this issue and improve the accuracy of our data, we decided to use a different set of polygons (parishes) with a lower resolution for the next project. We hoped that this approach would help to more accurately associate the postal codes with the intended polygons, resulting in a more reliable dataset.---
slug: infinite-universe
title: Infinite Universe
authors: ecolazy
tags: [hello, docusaurus]
---

# Magical Sky

![Docusaurus Plushie](/img/ms-3.png)

# Infinite Universe
## Method of Creating an Illusion for an Infinite Universe Utilizing LED Light Strips, Acrylic Sheets, Mirror Film, Half Mirror Film, and Dichroic Film

## Field of the Invention
The present invention relates to the field of visual effects and illusion creation, specifically pertaining to a method that employs LED light strips, acrylic sheets, mirror film, half mirror film, and dichroic film to generate an immersive illusion of an infinite universe.

## Background
Visual effects and illusions have long fascinated individuals, resulting in the development of various techniques and devices to engage and captivate viewers. However, there exists a need for an innovative method that provides an enhanced and immersive experience of an infinite universe. The present invention addresses this need by combining specific components in a unique arrangement to achieve a visually stunning illusion.

## Objective 
The invention aims to provide a method for creating an illusion of an infinite universe. By meticulously arranging and utilizing LED light strips, acrylic sheets, mirror film, half mirror film, and dichroic film, the invention produces a captivating visual experience that simulates an expansive cosmic environment.

## Statements of Invention 
The invention comprises the following components and their respective functions:
Layer 1: Optical Acrylic with LED Light Strip This layer consists of an optical-grade acrylic sheet featuring an integrated LED light strip running along one edge. The acrylic sheet is designed to possess optimal light reflection and transmission properties. The LED light strip illuminates the acrylic sheet, generating a radiant and uniform distribution of light.

Layer 2: Mirror Film with Star-Shaped Holes The second layer incorporates a mirror film with strategically positioned star-shaped holes on the front face of the acrylic sheet. These apertures enable light to escape in a diffused manner, emulating the appearance of stars. The mirror film enhances the reflective properties, amplifying the impact of the illusion.

Layer 3: Dichroic Film The third layer involves the integration of a dichroic film renowned for its ability to selectively reflect and transmit specific wavelengths of light. This film introduces vibrant colors and dynamic effects to the illusion, causing the stars to exhibit various hues.

Layer 4: Acrylic Sheet with 20% Transmissivity Mirror Coating The fourth layer encompasses another acrylic sheet coated with a mirror coating, allowing approximately 20% of light transmission. This semi-transparent mirror coating permits the perception of both reflected and partially transmitted light, contributing to the depth and complexity of the illusion while immersing the viewer in an expansive visual landscape.

## Description of Examples
The invention operates as follows: The LED light strip situated along the edge of the optical acrylic sheet provides uniform illumination across its surface. The light undergoes internal reflection within the acrylic sheet, aided by reflective white sheets covering its other faces. The star-shaped holes on the mirror film side of the acrylic sheet enable diffused light to escape, resulting in a celestial effect.

The escaping light passes through the dichroic film, which selectively reflects specific wavelengths of light, thereby causing the stars to appear in vibrant colors and adding depth to the illusion. The colored light is subsequently reflected onto the mirror coating of the entire assembly's front face. Due to the offset angle of the mirror coating, the reflected light creates the impression of an infinite universe extending beyond the boundaries of the assembly. This interplay of light, reflection, and color generates a captivating visual effect that enthralls the viewer and evokes a sense of expansiveness.

## Customization and Application
The method for creating the illusion of an infinite universe using the described components can be further optimized and customized. The arrangement, size, and positioning of the components can be adjusted to achieve specific visual effects and desired levels of immersion. Additional layers or elements may also be incorporated to enhance the overall illusion.
The described method can be applied in various contexts, including but not limited to entertainment venues, art installations, virtual reality experiences, and other visual display systems. Its versatility allows for implementation in diverse settings to create captivating and immersive visual experiences.

In conclusion, the method for creating an illusion of an infinite universe using LED light strips, acrylic sheets, mirror film, half mirror film, and dichroic film offers a unique and visually captivating experience. The meticulous arrangement and interplay of these components result in a mesmerizing illusion that simulates the vastness and beauty of an infinite cosmic environment.

It is important to note that the aforementioned description serves as an illustration of preferred embodiments of the invention. Those skilled in the art will recognize various modifications and changes that can be made to the described method while remaining within the scope and spirit of the invention. Therefore, the scope of the invention should not be limited to the specific embodiments described, but should be determined by the appended claims and their legal equivalents.




---
sidebar_position: 1
---

# Introduction

Welcome to the documentation of my previous work! This section is designed to provide detailed documentation of the projects and processes I have completed in the past.

The purpose of this documentation is multifaceted. Firstly, it serves as a reference for me to refer back to in the event that I need to repeat any of the steps or techniques used in my previous work. This is especially useful when the work was completed some time ago and I may not remember all of the details.

In addition to serving as a reference for myself, this documentation can also be used as a resource for others who may be interested in understanding my work or in replicating any of the techniques I have used. By providing detailed documentation, I aim to make my work more transparent and accessible to others.

Overall, this section is an important part of my professional practice as it helps me to document and preserve the knowledge and experience I have gained through my work. By keeping a record of my past projects and processes, I can continue to build upon my expertise and contribute to the field in a meaningful way.---
sidebar_position: 4
---

# Provisioning Fedora Workstation

This script installs and updates various packages and tools on a Fedora system. It installs the PostgreSQL database management system, the RPM Fusion repositories, and the Flathub repository. It also updates the system's packages and firmware, installs the Gnome Tweak Tool, some tools for working with CoreOS, and sets some configuration options for dnf (the Fedora package manager).

``` bash
#!/bin/bash

# Set Fedora version
FEDORA_VER=$(rpm -E %fedora)

# Remove any old pgadmin repos
sudo rpm -e pgadmin4-fedora-repo

# Set URLs for RPM Fusion repositories
FREE_REPO_URL="https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-${FEDORA_VER}.noarch.rpm"
NONFREE_REPO_URL="https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-${FEDORA_VER}.noarch.rpm"

# Set URL for pgAdmin4 repository
PGADMIN_REPO_URL="https://ftp.postgresql.org/pub/pgadmin/pgadmin4/yum/pgadmin4-fedora-repo-2-1.noarch.rpm"

# Set dnf configuration options
echo 'max_parallel_downloads=10' | sudo tee -a /etc/dnf/dnf.conf
echo 'fastestmirror=True' | sudo tee -a /etc/dnf/dnf.conf

# Add the PostgreSQL repository
sudo rpm -i "$PGADMIN_REPO_URL"

# Install RPM Fusion repositories
sudo dnf install "$FREE_REPO_URL" "$NONFREE_REPO_URL"

# Add Flathub repository
flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo

# Update and upgrade packages
sudo dnf update && sudo dnf upgrade

# Refresh package list and update core packages
sudo dnf upgrade --refresh
sudo dnf groupupdate core

# Update firmware
sudo fwupdmgr refresh --force
sudo fwupdmgr get-updates
sudo fwupdmgr update

# Install Gnome Tweak Tool and tool for working with CoreOS
sudo dnf install gnome-tweak-tool rpi-imager coreos-installer

# Install Qgis
sudo dnf install qgis python3-qgis qgis-grass qgis-server

# Install pgadmin4
sudo tum install pgadmin4

# Install ranger
sudo dnf ranger

# Install zsh
sudo dnf zsh

# Install kitty
sudo dnf kitty

# Install psql
sudo dnf psql

# Oh My ZSH
wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh
sh install.sh
```

---
sidebar_position: 5
---

# Configuring Visual Studio Code

### Generate list of extensions from existing installation
When this command is run, it will generate a list of all the installed Visual Studio Code extensions and save it to the extensions.md file. The extension names will be listed one per line in the file.

``` bash
code --list-extensions > extensions.md
```

Here's the resulting file
### extensions.md
``` md
aaron-bond.better-comments
bierner.markdown-preview-github-styles
donjayamanne.githistory
eamodio.gitlens
esbenp.prettier-vscode
formulahendry.auto-rename-tag
IvanGrigorov.openaicodehelper
jamiewoodio.cisco
TabNine.tabnine-vscode
voldemortensen.rainbow-tags
yzhang.markdown-all-in-one
```

### Install extensions from md file

The xargs command is used to build and execute a command from standard input. It takes the input, splits it into separate arguments (using the -n1 option, which means to take one argument at a time), and then runs the specified command (in this case, code --install-extension) on each argument.

The < extensions.md part of the command passes the contents of the extensions.md file as standard input to the xargs command.

So, in this case, the command reads the extensions.md file, which should contain a list of extension names (one per line), and it installs each extension using the code command.

``` bash
xargs -n1 code --install-extension < extensions.md
```

---
sidebar_position: 3
---

# Rename all linux frendly

This command will rename all the files in the current directory, replacing any non-alphanumeric or non-period characters with nothing.

``` bash
for file in *; do mv "$file" "$(echo ${file//[^a-zA-Z0-9.]/})" ; done
```

## References
---
sidebar_position: 2
---

# Extract all zips

This script will find all .zip files in the current directory and its subdirectories, and it will unzip each of them.

``` bash
#!/usr/bin/bash
find . -name "*.zip" -exec unzip {} \;
```



## References

---
sidebar_position: 1
---

# Copy files into directories by extension
This script searches all subdirectories for files and moves them into subdirectories named after their file extensions, while also handling filenames that contain special characters:

``` bash
find . -type f | while read -r filename; do
  base=$(basename "$filename" .*)
  ext=${filename##*.}
  counter=1
  sanitized_base=$(echo "$base" | tr -dc '[:alnum:]\n\r')
  sanitized_ext=$(echo "$ext" | tr -dc '[:alnum:]\n\r')
  while [[ -f "${sanitized_ext}/${sanitized_base}_${counter}.${sanitized_ext}" ]]; do
      ((counter++))
  done
  mkdir -p "${sanitized_ext}"
  mv "$filename" "${sanitized_ext}/${sanitized_base}_${counter}.${sanitized_ext}"
done


## now test there is files in the directories  




```

If a file with the same name already exists in the destination directory, it will append a number to the file name to make it unique. For example, if the file "example.txt" already exists in the "txt" directory, the script will move the new file "example.txt" to "txt/example_1.txt". If there is already a file "example_1.txt" in the "txt" directory, the script will move the new file to "txt/example_2.txt", and so on.

The script also sanitizes the file names to remove any special characters that might cause errors. It does this by using the tr command to remove all characters that are not alphanumeric or newline characters from the file names. This ensures that the resulting file names will only contain safe characters.


## References---
sidebar_position: 4
---

# Setup rclone

### Install rclone from the package manager
``` bash
sudo dnf install rclone
```

### Run the rclone configuration wizard to set up a remote storage provider
``` bash
rclone config
```


### Open the rclone service file in a text editor
``` bash
sudo vim ~/.config/systemd/user/rclone.service
```

### Add the following content to the service file
```
[Unit]
Description=Rclone Sync
After=network.target

[Service]
Type=simple
# Specify the sync command to be run at boot
ExecStart=/usr/bin/rclone sync /Documents/ google:Documents/ 
# Restart the service if it fails
Restart=on-failure

[Install]
# Enable the service to start at boot
WantedBy=multi-user.target
```

### Check the status of the rclone service
``` bash
systemctl --user status rclone.service
```

### Start the rclone service immediately
``` bash
systemctl --user start rclone.service
```

### Enable the rclone service to start at boot
``` bash
systemctl --user enable rclone.service
```

### Reload the system manager configuration
``` bash
systemctl --user daemon-reload
```---
sidebar_position: 5
---

# Setting up Qgis

``` bash
sudo dnf install qgis python3-qgis qgis-grass qgis-server
```


---
sidebar_position: 2
---

# Infrastructure as a Service

IaaS stands for "Infrastructure as a Service." It refers to a cloud computing model in which an organization outsources the infrastructure necessary to support the operation of its applications and services. This includes physical infrastructure, such as servers, storage, and networking, as well as virtual infrastructure, such as operating systems, middleware, and runtime environments. With IaaS, the organization pays for the infrastructure resources it consumes on a pay-as-you-go basis.---
sidebar_position: 3
---

# Platform as a Service

PaaS stands for "Platform as a Service." It refers to a cloud computing model in which an organization develops and deploys applications on a cloud-based platform provided by a third-party provider. The provider manages the infrastructure and middleware required to support the applications, allowing the organization to focus on developing and deploying the applications. The organization pays for the platform on a pay-as-you-go basis, typically based on the resources consumed by the applications.---
sidebar_position: 1
---

# Software as a Service

SaaS stands for "Software as a Service." It refers to a cloud computing model in which an organization uses software applications that are hosted and managed by a third-party provider. The organization pays for the software on a subscription basis, typically based on the number of users or the amount of data processed. With SaaS, the organization does not need to worry about installing, configuring, or maintaining the software, as these tasks are handled by the provider.---
sidebar_position: 2
---

# Setting up InfluxDB Database Server with Podman

First create a pod and inflluxdb container using podman, then generate a YAML file using podman play.

The YAML file can be used to recreate the pod in podman, or in kubernetes.

### Create a pod
``` bash
podman pod create -p 8086:8086 -p 1883:1883 -p 9001:9001 -n monitoring
```

### Pull influxdb image
``` bash
podman pull docker.io/influxdb:latest
```

### Run image in pod
``` bash
podman run -d -t \
--name influxdb \
--pod monitoring \
influxdb:latest
```

### Pull mosquitto image
``` bash
podman pull docker.io/eclipse-mosquitto
```

### Run image in pod
``` bash
podman run -t \
--name mosquitto \
--pod monitoring \
eclipse-mosquitto
```


### Generate YAML file
``` bash
podman generate kube monitoring -f monitoring-stack.yaml
```

## Here's the YAML file
``` yaml
# Save the output of this file and use kubectl create -f to import
# it into Kubernetes.
#
# Created with podman-4.3.1
apiVersion: v1
kind: Pod
metadata:
  annotations:
    io.kubernetes.cri-o.ContainerType/influxdb: container
    io.kubernetes.cri-o.ContainerType/mosquitto: container
    io.kubernetes.cri-o.SandboxID/influxdb: 3e6d0de4f62b7090a2b3e0e4d64f69881894d6d4988b4f87cde736c43e26a62
    io.kubernetes.cri-o.SandboxID/mosquitto: 3e6d0de4f62b7090a2b3e0e4d64f69881894d6d4988b4f87cde736c43e26a62
    io.kubernetes.cri-o.TTY/influxdb: "true"
    io.kubernetes.cri-o.TTY/mosquitto: "true"
    io.podman.annotations.autoremove/influxdb: "FALSE"
    io.podman.annotations.autoremove/mosquitto: "FALSE"
    io.podman.annotations.init/influxdb: "FALSE"
    io.podman.annotations.init/mosquitto: "FALSE"
    io.podman.annotations.privileged/influxdb: "FALSE"
    io.podman.annotations.privileged/mosquitto: "FALSE"
    io.podman.annotations.publish-all/influxdb: "FALSE"
    io.podman.annotations.publish-all/mosquitto: "FALSE"
  creationTimestamp: "2022-12-31T02:23:51Z"
  labels:
    app: monitoring
  name: monitoring
spec:
  automountServiceAccountToken: false
  containers:
  - args:
    - influxd
    image: docker.io/library/influxdb:latest
    name: influxdb
    ports:
    - containerPort: 1883
      hostPort: 1883
    - containerPort: 8086
      hostPort: 8086
    - containerPort: 9001
      hostPort: 9001
    resources: {}
    securityContext:
      capabilities:
        drop:
        - CAP_MKNOD
        - CAP_NET_RAW
        - CAP_AUDIT_WRITE
    tty: true
    volumeMounts:
    - mountPath: /etc/influxdb2
      name: 130b34101cdf2ca9f58b6166ea376a0f79c5fe18889a00c42f458d3259a8fd8e-pvc
    - mountPath: /var/lib/influxdb2
      name: 70d043a228ff0ccb924b9950887e9b947cfebed5956652cbf2d454ac26a66879-pvc
  - args:
    - /usr/sbin/mosquitto
    - -c
    - /mosquitto/config/mosquitto.conf
    image: docker.io/library/eclipse-mosquitto:latest
    name: mosquitto
    resources: {}
    securityContext:
      capabilities:
        drop:
        - CAP_MKNOD
        - CAP_NET_RAW
        - CAP_AUDIT_WRITE
    tty: true
    volumeMounts:
    - mountPath: /mosquitto/log
      name: f01fc59a9790fea52fd1e862b517d10aaefa3af04ad70e912004b72fd64e95b0-pvc
    - mountPath: /mosquitto/data
      name: 3439da031be88b060d2c4fb4c835a309c573809fcfb45f98dfc12e281f6d263e-pvc
  enableServiceLinks: false
  hostname: monitoring
  restartPolicy: Never
  volumes:
  - name: 130b34101cdf2ca9f58b6166ea376a0f79c5fe18889a00c42f458d3259a8fd8e-pvc
    persistentVolumeClaim:
      claimName: 130b34101cdf2ca9f58b6166ea376a0f79c5fe18889a00c42f458d3259a8fd8e
  - name: 70d043a228ff0ccb924b9950887e9b947cfebed5956652cbf2d454ac26a66879-pvc
    persistentVolumeClaim:
      claimName: 70d043a228ff0ccb924b9950887e9b947cfebed5956652cbf2d454ac26a66879
  - name: f01fc59a9790fea52fd1e862b517d10aaefa3af04ad70e912004b72fd64e95b0-pvc
    persistentVolumeClaim:
      claimName: f01fc59a9790fea52fd1e862b517d10aaefa3af04ad70e912004b72fd64e95b0
  - name: 3439da031be88b060d2c4fb4c835a309c573809fcfb45f98dfc12e281f6d263e-pvc
    persistentVolumeClaim:
      claimName: 3439da031be88b060d2c4fb4c835a309c573809fcfb45f98dfc12e281f6d263e
status: {}
```

### Test YAML file

Delete containers

``` bash
podman rm -vf influxdb
```
``` bash
podman rm -vf mosquitto
```
Delete pod

``` bash
podman pod rm monitoring
```

Re-build pod using podman play and YAML file.

``` bash
podman play kube monitoring-stack.yaml
```

### Reference
[Oracle-Base](https://oracle-base.com/articles/linux/podman-generate-and-play-kubernetes-yaml-files#:~:text=Podman%20can%20generate%20Kubernetes%20YAML,similar%20to%20Docker%20Compose%20files.)





### To do
- mosquitto configuration
- influxdb directory---
sidebar_position: 1
---

# Provisioning Fedora CoreOS on the Raspberry Pi 4
### prep seperate usb for firmware
``` bash
VERSION=v1.32  # use latest one from https://github.com/pftf/RPi4/releases
UEFIDISK=/dev/sdX
sudo mkfs.vfat $UEFIDISK
mkdir /tmp/UEFIdisk
sudo mount $UEFIDISK /tmp/UEFIdisk
pushd /tmp/UEFIdisk
sudo curl -LO https://github.com/pftf/RPi4/releases/download/${VERSION}/RPi4_UEFI_Firmware_${VERSION}.zip
sudo unzip RPi4_UEFI_Firmware_${VERSION}.zip
sudo rm RPi4_UEFI_Firmware_${VERSION}.zip
popd
sudo umount /tmp/UEFIdisk
```

### Install CoreOS tools
``` bash
sudo dnf install -y rpi-imager coreos-installer butane ignition-validate
```

### Make working directory and change to it
``` bash
mkdir ~/coreos
cd ~/coreos
``` 

### Download CoreOS image
``` bash
coreos-installer download -p qemu -f qcow2.xz --decompress
```

### Rename image to simpler name
``` bash
mv *.qcow2 fedora-coreos.qcow2
```

### Create rpict.bu 
``` yaml
variant: fcos
version: 1.4.0
passwd:
  users:
    - name: core
      ssh_authorized_keys:
        - ssh-rsa AAAA...
systemd:
  units:
    - name: serial-getty@ttyS0.service
      dropins:
      - name: autologin-core.conf
        contents: |
          [Service]
          # Override Execstart in main unit
          ExecStart=
          # Add new Execstart with `-` prefix to ignore failure
          ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM
          TTYVTDisallocate=no
    - name: failure.service
      enabled: true
      contents: |
        [Service]
        Type=oneshot
        ExecStart=/usr/bin/false
        RemainAfterExit=yes

        [Install]
        WantedBy=multi-user.target
    - name: etcd-member.service
      enabled: true
      contents: |
        [Unit]
        Description=Run a single node etcd
        After=network-online.target
        Wants=network-online.target

        [Service]
        ExecStartPre=mkdir -p /var/lib/rpict2mqtt
        ExecStartPre=-/bin/podman kill rpict2mqtt
        ExecStartPre=-/bin/podman rm rpict2mqtt
        ExecStartPre=-/bin/podman pull docker.io/gtricot/rpict-mqtt:latest
        ExecStart=/bin/podman run --name rpict2mqtt \
                           --device=/dev/ttyAMA0:/dev/ttyAMA0 \
                            -e MQTT_URL="mqtt://my_mqtt_broker:1883" \
                           -e MQTT_USER="my-super-user" \
                           -e MQTT_PASSWORD="my-secret-password" \
                           -e MQTT_BASE_TOPIC="custom-rpict-topic" \
                           -e ABSOLUTE_VALUES=true \
                           -e SENSOR_VALUE_THRESHOLD=2 \
                            gtricot/rpict-mqtt
        ExecStop=/bin/podman stop rpict2mqtt

        [Install]
        WantedBy=multi-user.target
storage:
  files:
    - path: /etc/hostname
      mode: 0644
      contents:
        inline: |
          tutorial
    - path: /etc/profile.d/systemd-pager.sh
      mode: 0644
      contents:
        inline: |
          # Tell systemd to not use a pager when printing information
          export SYSTEMD_PAGER=cat
```

### Transpile butane file into an ignition file
``` bash
butane --pretty --strict rpict.bu --output rpict.ign
```

## Test ignition file in virtual machine
``` bash
ignition-validate rpict.ign && echo 'Success!'
```

#### Setup the correct SELinux label to allow access to the config
``` bash
chcon --verbose --type svirt_home_t rpict.ign
```

#### Start a Fedora CoreOS virtual machine
``` bash
virt-install --name=fcos --vcpus=2 --ram=2048 --os-variant=fedora-coreos-stable \
    --import --network=bridge=virbr0 --graphics=none \
    --qemu-commandline="-fw_cfg name=opt/com.coreos/config,file=${PWD}/rpict.ign" \
    --disk=size=20,backing_store=${PWD}/fedora-coreos.qcow2
```
#### Exit and destroy virtual machine 
> CTRL + ] to exit kvm
### to destroy run
``` bash
virsh destroy fcos
virsh undefine --remove-all-storage fcos
```

### Write to disk
``` bash
## set disc
FCOSDISK=/dev/sdX
```

``` bash
# Create customized.iso which:
coreos-installer iso customize \
    --architecture=aarch64 \
    --dest-device $FCOSDISK \ # - Automatically installs to /dev/sda
    --dest-ignition config.ign \ # - Provisions with config.ign
    --network-keyfile $networkManagerConnectionFile  \ # -  network configuration
    --ignition-ca ca.pem \ # - Trusts HTTPS certificates signed by ca.pem
    --post-install post.sh \ # - Runs post.sh after installing
    -o custom.iso input.iso
```

## Reference
[FedoraOnRpi](https://docs.fedoraproject.org/en-US/fedora-coreos/provisioning-raspberry-pi4/)

[customizing-install](https://coreos.github.io/coreos-installer/customizing-install/#customize-options)

[pftf](https://github.com/pftf/RPi4)



### To do
- setup MQTT gateway
- write CoreOS to SD and boot pi with external monitor
- test whether /dev/ttyAMA0 is accesible with CoreOS on RPI4---
sidebar_position: 3
---

# Carbon Dioxide

### Hardware

> MH-Z19B NDIR infrared gas module is a common type, small size sensor, using non-dispersive infrared (NDIR) principle to detect the existence of CO 2 in the air, with good selectivity, non-oxygen dependent and long life. Built-in temperature compensation; and it has UART output and PWM output. It is developed by the tight integration of mature infrared absorbing gas detection technology, precision optical circuit design and superior circuit design.

General
 - MH-Z19B Carbon Dioxide Gas Sensor
 - uses the principle of non-scattered infrared
 - simultaneous serial, analog and PWM output 

### Script

``` python
#!/usr/bin/python3

import socket
import ssl
import sys
import re
import json
import os.path
import argparse
from time import time, sleep, localtime, strftime
from colorama import init as colorama_init
from colorama import Fore, Back, Style
from configparser import ConfigParser
from unidecode import unidecode
import mh_z19
import paho.mqtt.client as mqtt
import sdnotify

project_name = 'MH-Z19 Raspberry MQTT Client/Daemon'
project_url = 'https://github.com/R4scal/mhz19-mqtt-daemon'

if False:
    # will be caught by python 2.7 to be illegal syntax
    print('Sorry, this script requires a python3 runtime environemt.', file=sys.stderr)


# Argparse
parser = argparse.ArgumentParser(description=project_name, epilog='For further details see: ' + project_url)
parser.add_argument('--config_dir', help='set directory where config.ini is located', default=sys.path[0])
parse_args = parser.parse_args()

# Intro
colorama_init()
print(Fore.GREEN + Style.BRIGHT)
print(project_name)
print('Source:', project_url)
print(Style.RESET_ALL)

# Systemd Service Notifications - https://github.com/bb4242/sdnotify
sd_notifier = sdnotify.SystemdNotifier()

# Logging function
def print_line(text, error = False, warning=False, sd_notify=False, console=True):
    timestamp = strftime('%Y-%m-%d %H:%M:%S', localtime())
    if console:
        if error:
            print(Fore.RED + Style.BRIGHT + '[{}] '.format(timestamp) + Style.RESET_ALL + '{}'.format(text) + Style.RESET_ALL, file=sys.stderr)
        elif warning:
            print(Fore.YELLOW + '[{}] '.format(timestamp) + Style.RESET_ALL + '{}'.format(text) + Style.RESET_ALL)
        else:
            print(Fore.GREEN + '[{}] '.format(timestamp) + Style.RESET_ALL + '{}'.format(text) + Style.RESET_ALL)
    timestamp_sd = strftime('%b %d %H:%M:%S', localtime())
    if sd_notify:
        sd_notifier.notify('STATUS={} - {}.'.format(timestamp_sd, unidecode(text)))

# Eclipse Paho callbacks - http://www.eclipse.org/paho/clients/python/docs/#callbacks
def on_connect(client, userdata, flags, rc):
    if rc == 0:
        print_line('MQTT connection established', console=True, sd_notify=True)
        print()
    else:
        print_line('Connection error with result code {} - {}'.format(str(rc), mqtt.connack_string(rc)), error=True)
        #kill main thread
        os._exit(1)

def on_publish(client, userdata, mid):
    #print_line('Data successfully published.')
    pass


# Load configuration file
config_dir = parse_args.config_dir

config = ConfigParser(delimiters=('=', ))
config.optionxform = str
config.read([os.path.join(config_dir, 'config.ini.dist'), os.path.join(config_dir, 'config.ini')])

reporting_mode = config['General'].get('reporting_method', 'homeassistant-mqtt')
daemon_enabled = config['Daemon'].getboolean('enabled', True)
sleep_period = config['Daemon'].getint('period', 300)
detection_range = config['MH-Z19'].getint('detection_range', 5000)

if reporting_mode == 'homeassistant-mqtt':
    default_base_topic = 'homeassistant'

base_topic = config['MQTT'].get('base_topic', default_base_topic).lower()

# Check configuration
if reporting_mode not in ['homeassistant-mqtt']:
    print_line('Configuration parameter reporting_mode set to an invalid value', error=True, sd_notify=True)
    sys.exit(1)

print_line('Configuration accepted', console=False, sd_notify=True)

# MQTT connection
if reporting_mode in ['homeassistant-mqtt']:
    print_line('Connecting to MQTT broker ...')
    mqtt_client = mqtt.Client()
    mqtt_client.on_connect = on_connect
    mqtt_client.on_publish = on_publish

    if config['MQTT'].getboolean('tls', False):
        mqtt_client.tls_set(
            ca_certs=config['MQTT'].get('tls_ca_cert', None),
            keyfile=config['MQTT'].get('tls_keyfile', None),
            certfile=config['MQTT'].get('tls_certfile', None),
            # Auto-negotiate the highest protocol version that both the client and server support, and configure the
            # context client-side connections. Other protocol options are deprecated
            tls_version=ssl.PROTOCOL_TLS_CLIENT
        )

    if config['MQTT'].get('username'):
        mqtt_client.username_pw_set(config['MQTT'].get('username'), config['MQTT'].get('password', None))
    try:
        mqtt_client.connect(config['MQTT'].get('hostname', 'localhost'),
                            port=config['MQTT'].getint('port', 1883),
                            keepalive=config['MQTT'].getint('keepalive', 60))
    except:
        print_line('MQTT connection error. Please check your settings in the configuration file "config.ini"', error=True, sd_notify=True)
        sys.exit(1)
    else:
       mqtt_client.loop_start()
       sleep(1.0) # some slack to establish the connection

sd_notifier.notify('READY=1')

# Initialize DHT sensor
sensor_name = '{}_mhz19'.format(socket.gethostname()).replace("-", "_")
print_line('Current sensor name is "{}"'.format(sensor_name).lower())

# Discovery Announcement
if reporting_mode == 'homeassistant-mqtt':
    print_line('Announcing MH-Z19 to MQTT broker for auto-discovery ...')
    topic_path = '{}/sensor/{}'.format(base_topic, sensor_name)
    base_payload = {
        "state_topic": "{}/state".format(topic_path).lower()
    }
    # Temperature
    payload = dict(base_payload.items())
    payload['unit_of_measurement'] = 'C'
    payload['value_template'] = "{{ value_json.temperature }}"
    payload['name'] = "{} Temperature".format(sensor_name)
    payload['device_class'] = 'temperature'
    mqtt_client.publish('{}/{}_temperature/config'.format(topic_path, sensor_name).lower(), json.dumps(payload), 1, True)
    # CO2
    payload = dict(base_payload.items())
    payload['unit_of_measurement'] = 'ppm'
    payload['value_template'] = "{{ value_json.co2 }}"
    payload['name'] = "{} CO2".format(sensor_name)
    mqtt_client.publish('{}/{}_co2/config'.format(topic_path, sensor_name).lower(), json.dumps(payload), 1, True)
    # SS
    payload = dict(base_payload.items())
    payload['unit_of_measurement'] = ''
    payload['value_template'] = "{{ value_json.SS }}"
    payload['name'] = "{} SS".format(sensor_name)
    mqtt_client.publish('{}/{}_ss/config'.format(topic_path, sensor_name).lower(), json.dumps(payload), 1, True)
    # UhUl
    payload = dict(base_payload.items())
    payload['unit_of_measurement'] = ''
    payload['value_template'] = "{{ value_json.UhUl }}"
    payload['name'] = "{} UhUl".format(sensor_name)
    mqtt_client.publish('{}/{}_uhul/config'.format(topic_path, sensor_name).lower(), json.dumps(payload), 1, True)


if detection_range == 5000:
    mh_z19.detection_range_5000(serial_console_untouched=True)
elif detection_range == 10000:
    mh_z19.detection_range_10000(serial_console_untouched=True)
elif detection_range == 2000:
    mh_z19.detection_range_2000(serial_console_untouched=True)
else:
    # Unknown detection range, setting to 5000
    mh_z19.detection_range_5000(serial_console_untouched=True)

# Sensor data retrieval and publication
while True:
   print_line('Retrieving data from MH-Z19 sensor...')
   data = mh_z19.read_all(serial_console_untouched=True)
   if len(data) == 0:
      print_line('Unable to get data form sensor.', error=True, sd_notify=True)
      print()
      continue
   else:
     print_line('Result: {}'.format(json.dumps(data)))
     if reporting_mode == 'homeassistant-mqtt':
          print_line('Publishing to MQTT topic "{}/sensor/{}/state"'.format(base_topic, sensor_name).lower())
          mqtt_client.publish('{}/sensor/{}/state'.format(base_topic, sensor_name).lower(), json.dumps(data))
          sleep(0.5) # some slack for the publish roundtrip and callback function
     else:
          raise NameError('Unexpected reporting_mode.')
     print()

     print_line('Status messages published', console=False, sd_notify=True)

   if daemon_enabled:
      print_line('Sleeping ({} seconds) ...'.format(sleep_period))
      sleep(sleep_period)
      print()
   else:
      print_line('Execution finished in non-daemon-mode', sd_notify=True)
      break
```

## References

[Wikipedia](https://en.wikipedia.org/wiki/Infrared_spectroscopy)
[Winsen-Sensors](https://www.winsen-sensor.com/sensors/co2-sensor/mh-z19b.html)
[Code](https://github.com/R4scal/mhz19-mqtt-daemon)---
sidebar_position: 1
---

# Starting Scripts with Systemd

``` bash
cp script.py /usr/local/bin/script.py
```

``` service
[Unit]
Before=systemd-user-sessions.service
Wants=network-online.target
After=network-online.target
ConditionPathExists=!/var/lib/issuegen-public-ipv4

[Service]
Type=oneshot
ExecStart=/usr/local/bin/public-ipv4.sh
ExecStartPost=/usr/bin/touch /var/lib/issuegen-public-ipv4
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target

```



## References

[RPICT7V1](https://docs.fedoraproject.org/en-US/fedora-coreos/tutorial-services/)

---
sidebar_position: 1
---

# AC Current

### Hardware

- RPICT7V1 AC current sensor 
- SCT-013-000 100a Current Transformer
- UK: 77DB-06-09 Voltage Sensor

### Script

``` python
MQTT_SERV = "localhost"
MQTT_PATH = "RPICT7V1"
MQTT_USER = ""
MQTT_PASS = ""

CHANNELS = ["NodeID", "RP1", "RP2", "RP3", "RP4", "RP5", "RP6", "RP7",
		"Irms1", "Irms2", "Irms3", "Irms4", "Irms5", "Irms6", "Irms7",
		"Vrms"]

import paho.mqtt.client as mqtt
import serial
ser = serial.Serial('/dev/ttyAMA0', 38400)

client = mqtt.Client("P1")
client.username_pw_set(MQTT_USER, MQTT_PASS)
client.connect(MQTT_SERV)

try:
 	while 1:
 		# Read one line from the serial buffer
		 line = ser.readline()
	 
	 	# Remove the trailing carriage return line feed
	 	line = line[:-2]
	 
	 	# Create an array of the data
	 	Z = line.split(' ')
	 
	 	# Print it for debug
	 	print line
	 
		# Publish to the MQTT broker
 		for i in range(len(Z)):
 			client.publish("%s/%s" % (MQTT_PATH, CHANNELS[i]), Z[i]) 
 
except KeyboardInterrupt:
	client.disconnect()
	ser.close()
```


## References

[RPICT7V1](http://lechacal.com/wiki/index.php?title=Raspberrypi_Current_and_Temperature_Sensor_Adaptor#RPICT_Series)---
sidebar_position: 2
---

# Car OBD Port

### Hardware

> OBD-II PIDs (On-board diagnostics Parameter IDs) are codes used to request data from a vehicle, used as a diagnostic tool.

### Script

``` python
#!/usr/bin/python3
import obd
import time
import json
import paho.mqtt.client as mqtt

# define variables
connection = obd.Async("/dev/ttyUSB0")

# global variables to hold values
latest_speed = {}
latest_rpm = {}
latest_engine_load = {}
latest_coolant_temp = {}
latest_intake_pressure = {}
latest_intake_temp = {}
latest_maf = {}
latest_distance_w_mil = {}
latest_fuel_rail_pressure_direct = {}
latest_commanded_egr = {}
latest_fuel_level = {}
latest_barometric_pressure = {}

# loop
def myLoop():
    message = [
                {
                    "measurement": "obd",
                        "fields": {
                            "commanded_egr": latest_commanded_egr,
                            "speed": latest_speed,
                            "rpm": latest_rpm,
                            "engine_load": latest_engine_load,
                            "coolant_temp": latest_coolant_temp,
                            "intake_pressure": latest_intake_pressure,
                            "intake_temp": latest_intake_temp,                            
                            "maf": latest_maf,
                            "distance_w_mil": latest_distance_w_mil,
                            "fuel_rail_pressure_direct": latest_fuel_rail_pressure_direct,
                            "commanded_egr": latest_commanded_egr,
                            "fuel_level": latest_fuel_level,
                            "barometric_pressure": latest_barometric_pressure
                    }
            }
            ]
    # convert to json string
    data_out=json.dumps(message) 

    # This is the Publisher  
    client = mqtt.Client()
    client.username_pw_set(username="admin",password="35d8e")
    client.connect("localhost",1883,60)
    client.publish("sensors/obd", data_out);
    client.disconnect();
    time.sleep(3)

# callbacks for each sensor
def new_speed(r):
    global latest_speed
    latest_speed = r.value.magnitude
    
def new_rpm(r):
    global latest_rpm
    latest_rpm = r.value.magnitude

def new_engine_load(r):
    global latest_engine_load
    latest_engine_load = r.value.magnitude
   
def new_coolant_temp(r):
    global latest_coolant_temp
    latest_coolant_temp = r.value.magnitude
    
def new_intake_pressure(r):
    global latest_intake_pressure
    latest_intake_pressure = r.value.magnitude
    
def new_intake_temp(r):
    global latest_intake_temp
    latest_intake_temp = r.value.magnitude
    
def new_maf(r):
    global latest_maf
    latest_maf = r.value.magnitude
    
def new_distance_w_mil(r):
    global latest_distance_w_mil
    latest_distance_w_mil = r.value.magnitude
    
def new_fuel_rail_pressure_direct(r):
    global latest_fuel_rail_pressure_direct
    latest_fuel_rail_pressure_direct =  r.value.magnitude
    
def new_commanded_egr(r):
    global latest_commanded_egr
    latest_commanded_egr = r.value.magnitude
    
def new_fuel_level(r):
    global latest_fuel_level
    latest_fuel_level = r.value.magnitude
    
def new_barometric_pressure(r):
    global latest_barometric_pressure
    latest_barometric_pressure = r.value.magnitude
   
# callbacks will fire upon receipt of new values
connection.watch(obd.commands.SPEED, callback=new_speed)
connection.watch(obd.commands.RPM, callback=new_rpm)
connection.watch(obd.commands.ENGINE_LOAD, callback=new_engine_load)
connection.watch(obd.commands.COOLANT_TEMP, callback=new_coolant_temp)
connection.watch(obd.commands.INTAKE_PRESSURE, callback=new_intake_pressure)
connection.watch(obd.commands.INTAKE_TEMP, callback=new_intake_temp)
connection.watch(obd.commands.MAF, callback=new_maf)
connection.watch(obd.commands.DISTANCE_W_MIL, callback=new_distance_w_mil)
connection.watch(obd.commands.FUEL_RAIL_PRESSURE_DIRECT, callback=new_fuel_rail_pressure_direct)
connection.watch(obd.commands.COMMANDED_EGR, callback=new_commanded_egr)
connection.watch(obd.commands.FUEL_LEVEL, callback=new_fuel_level)
connection.watch(obd.commands.BAROMETRIC_PRESSURE, callback=new_barometric_pressure)

# start connection
connection.start()

while True:
    myLoop()
    time.sleep(3)

time.sleep(60000)
connection.stop()
```

## References

[Wikipedia](https://en.wikipedia.org/wiki/OBD-II_PIDs)
[GitHub](https://github.com/VilmaH/Python-OBD-MQTT/blob/master/obdmqtt.py)---
sidebar_position: 3
---

# Thermocouple

### Hardware

### Script

``` python
import time
import board
import busio
import digitalio
import adafruit_max31855
from influxdb import InfluxDBClient

dbClient = InfluxDBClient('192.168.88.48', 8086, 'root', 'root', 'workshop')


spi = busio.SPI(board.SCK, MOSI=board.MOSI, MISO=board.MISO)
cs = digitalio.DigitalInOut(board.D5)

max31855 = adafruit_max31855.MAX31855(spi, cs)

while True:
    try:
        tempC = max31855.temperature
        tempF = tempC * 9 / 5 + 32
        print("Temperature: {} C {} F ".format(tempC, tempF))
        loginEvents = [{"measurement":"kiln",
                    "fields": {
                            "temp": tempC
                            }
                  }
             ]
        dbClient.write_points(loginEvents)
        time.sleep(2.0)
    except:
        pass
```


## References

[Adafruit](https://www.adafruit.com/product/269)---
sidebar_position: 2
---

# Installing Cisco Cloud Wireless Controller with KVM

## Cisco c9800-CL with KVM

### What?

The Cisco Catalyst c9800-CL is a wireless controller that is part of the Cisco Catalyst 9800 series. It is designed to manage and secure wireless networks, and provides features such as wireless intrusion prevention, location services, and guest access. The c9800-CL model is a cloud-based controller that is designed to be deployed in a virtual environment, and can be used to manage both on-premises and cloud-based wireless networks.

### Install virtualization sowftware group
``` bash
sudo dnf group install --with-optional virtualization
```

### Enable libvirtd service
``` bash
sudo systemctl start libvirtd && sudo systemctl enable libvirtd
```



## Create network bridge br10
???

## Install Virtual Machine
``` bash
virt-install \
--connect=qemu:///system \
--os-variant=rhel4.0 \
--arch=x86_64 \
--cpu host \
--console pty,target_type=virtio \
--hvm \
--import \
--name=my_c9k_vm \
--disk path=C9800-CL.qcow2,bus=ide,format=qcow2,backing_store \
--vcpus=1,sockets=1,cores=1,threads=1 \
--ram=4096 \
--network=network:model=virtio \
--network=network:br10,model=virtio \
--network=network:model=virtio  \
--noreboot 
```
> We use the backing_store option to virt-install --disk to quickly create a new disk image and avoid writing to the original image we have downloaded. This new disk image can be easily thrown away.
> 
### To exit KVM
CTRL + ] 

### If you need to start again, use this to destroy the VM
``` bash
virsh destroy fcos
virsh undefine --remove-all-storage fcos
```

### Configure the controller

The wireless controller is configured with this script. It sets the hostname of the controller to "9800-1" and creates a user with the name "admin" and password "Cisco123", giving the user privilege level 15. The Gigabit Ethernet interfaces 1 and 2 are then configured, with interface 1 being set up with a static IP address and interface 2 being set up as a trunk port with native VLAN 77. VLAN 77 is created and assigned an IP address, and static routes for the 10.10.10.0/24 and 0.0.0.0/0 networks are set up. The 5 GHz and 2.4 GHz radios on the controller are shut down, the country code is set to Great Britain, and the radios are re-enabled. Finally, the virtual wireless LAN controller (VWLC) is configured and the DNS server is set to 1.1.1.1 and the NTP server to pool.ntp.org.

``` ios
conf t
hostname 9800-1
user-name admin
 privilege 15
 password 0 Cisco123
 exit
int gig 1
 no switchport
 ip address 10.10.10.10 255.255.255.0
 no shut
 exit
int gig 2
 switchport
 switchport mode trunk
 switchport trunk native vlan 77
 no shut
 exit
int vlan 77
 ip address 192.168.77.10 255.255.255.0
 no shut
 exit
ip route 10.10.10.0 255.255.255.0 10.10.10.1
ip route 0.0.0.0 0.0.0.0 192.168.77.1
wireless management interface vlan 77
ap dot11 5ghz shutdown 
ap dot11 24ghz shutdown 
ap country GB
no ap dot11 5ghz shutdown
no ap dot11 24ghz shutdown
exit
wireless config vwlc-ssc key-size 2048 signature-algo sha256 password 0 Cisco123
conf t
ip name-server 1.1.1.1
ntp server pool.ntp.org
```

### Access the GUI

Now the GUI can be accessed at 192.168.77.1, login and go through the sero day configuration steps to setup a wireless network.

## References

[FedoraVirtGuide](https://docs.fedoraproject.org/en-US/quick-docs/getting-started-with-virtualization/)

[CiscoGuide](https://www.cisco.com/c/en/us/td/docs/wireless/controller/9800/9800-cloud/installation/b-c9800-cl-install-guide/installing_the_controller_in_kvm_environment.html)

[CiscoSal](https://youtu.be/6ttSeDTODWM)

[wireless_boi](https://youtu.be/MeDwvj0LxhU)---
sidebar_position: 4
---

# Translating books with DeepL

### Steps
- cut off spine of book
- scan in evey page
- combine pages into pdf
- split pdf into small pieces
- convert pdf pieces into word documents keeping the images in place
- upload word documents to DeepL translation
- convert returned translated word documents back into pdf
- combine all translated pdf pieces back into single translated pdf
- print


---
sidebar_position: 4
---

# Using OpenAI API
---
sidebar_position: 4
---

# Sun Shading Model With Radiance

To use Radiance to model sun shading, you will need to follow these steps:

Install Radiance: First, make sure that Radiance is installed on your computer. You can download Radiance from the official website (https://radiance-online.org/) or from a package manager such as Homebrew.

Create a 3D model of the scene: Use a 3D modeling software such as Rhino or SketchUp to create a 3D model of the building, terrain, and any other objects in the scene. You will need to export the model as a Radiance geometry file (.obj) or a Radiance scene description file (.rad).

Set up the Radiance simulation: Use a text editor to create a Radiance simulation script that specifies the input files, simulation parameters, and output settings. The script should include commands to define the location, time, and sky conditions, as well as the materials, textures, and geometry of the objects in the scene. You can refer to the Radiance documentation for more information on the available commands and options.

Run the Radiance simulation: Use the command-line interface to run the Radiance simulation using the script that you created. The simulation will generate a series of output files, including image files that show the shading patterns on the objects in the scene.

Analyze the results: Use a software tool such as ImageJ or GIMP to view and analyze the output image files. You can measure the amount of shading at different points on the objects, or you can use color mapping to visualize the shading patterns.---
sidebar_position: 4
---

# Modeling a spherical solar concentrator with Lady Bug

To use Radiance to model sun shading, you will need to follow these steps:

Install Radiance: First, make sure that Radiance is installed on your computer. You can download Radiance from the official website (https://radiance-online.org/) or from a package manager such as Homebrew.

Create a 3D model of the scene: Use a 3D modeling software such as Rhino or SketchUp to create a 3D model of the building, terrain, and any other objects in the scene. You will need to export the model as a Radiance geometry file (.obj) or a Radiance scene description file (.rad).

Set up the Radiance simulation: Use a text editor to create a Radiance simulation script that specifies the input files, simulation parameters, and output settings. The script should include commands to define the location, time, and sky conditions, as well as the materials, textures, and geometry of the objects in the scene. You can refer to the Radiance documentation for more information on the available commands and options.

Run the Radiance simulation: Use the command-line interface to run the Radiance simulation using the script that you created. The simulation will generate a series of output files, including image files that show the shading patterns on the objects in the scene.

Analyze the results: Use a software tool such as ImageJ or GIMP to view and analyze the output image files. You can measure the amount of shading at different points on the objects, or you can use color mapping to visualize the shading patterns.---
sidebar_position: 4
---

# Query workings

Identify mines within 2km of given point.

``` sql
select * 
from osmm_topo.cartographictext 
WHERE ST_DWithin(cartographictext.wkb_geometry,  ST_Transform(ST_GeomFromText(
              'POINT(  -2.2199 51.69382 )',4326), 27700) , 2000.0)
AND cartographictext.textstring like '%Workings%';
```

## References---
sidebar_position: 7
---
# Wildlife corridor overview

# Aims
- Identify areas of high connectivity between different land cover types, such as forests and grasslands. These areas are likely to be used by wildlife as corridors for movement.

- Identify potential barriers to wildlife movement, such as roads or urban areas



### Show column names
``` sql
SELECT COLUMN_NAME
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_NAME = 'topographicarea'
```

### Show entries in descriptivegroup column
``` sql
SELECT descriptivegroup
FROM topographicarea
```

### Show entries in column which match Natural Environment in descriptivegroup column
``` sql
SELECT *
FROM topographicarea
WHERE 'Natural Environment' = ANY (descriptivegroup)
```

### Show entries in column which match Road in descriptivegroup column
``` sql
SELECT *
FROM topographicarea
WHERE 'Road' = ANY (descriptiveterm)
```

### Show entries in column which match Building in descriptivegroup column

``` sql
SELECT *
FROM topographicarea
WHERE 'Building' = ANY (descriptivegroup)
```

![Docusaurus Plushie](/img/wildlife-corridors-1.png)

![Docusaurus Plushie](/img/wildlife-corridors-2.png)

You can see how the edges or roads and railway tracs could be used as wildlife corridors.

![Docusaurus Plushie](/img/wildlife-corridors-3.png)


## References---
sidebar_position: 3
---

# Land registry price paid parish

## Download data

We downloaded a CSV file that contained property price data for the past three years, along with the postal code coordinates and boundary line polygons.

```
$ wget bdline_gpkg_gb.zip
&& unzip bdline_gpkg_gb.zip
&& cd data
```
## Import bdline

We used ogr2ogr to convert a file containing boundary lines (in a format called GeoPackage) into a PostgreSQL file, changed the projection of the data from OSGB1936 to WGS84, and imported it into a database.

```
ogr2ogr \
-f "PostgreSQL" \
-a_srs "EPSG:27700" \
```

```
-t_srs "EPSG:4326" \
-progress PG:"dbname='gis' host='$ip' port='5432' user='$user'
password='$password'" \
bdline_gb.gpkg
```
## Connect to server

Starting a psql instance on the client in order to interact with the database on the server.

```
psql -h 192.168.88.10 -U postgres gis
```
# Create priced paid polygons for every point

We used point data that was already in the database from a previous project to create a new polygon for each point that was within the boundaries of a parish. We also added the price paid for each house (the point) to the corresponding polygon.

### SELECT

```
parish.geom,
points.pounds
INTO pp_parish
FROM
parish INNER JOIN points
ON st_contains(parish.geom, points.geom);
```
## Find avarage point value for duplicate polygons

Like in the the previous project, we calculated the average value for each of the duplicate polygons.

```
SELECT geom,avg(pounds)
INTO avg_pp_parish
FROM pp_parish
GROUP BY geom;
```
## Import new price paid polygons to file

```
Qgis > Database > DB Manager > Import Layer/File - Name: pp_parish
```
## Add price paid polygons layer to Qgis

```
Qgis > Layer > Add Layer > Add Vector Layer
Qgis > Database > DB Manager > Import Layer/File - Name: pp_parish
Vector Dataset(s): .shp
```

## Colour polygons by attribute field

```
Right click: Layer > Properties
Symbology > Single Symbol: Gradiated
Vaule: pounds
Colour Ramp: Spectral
Invert Colour Ramp
Segmentation: Equal Interval
```

![Docusaurus Plushie](/img/price-paid-parish.jpg)---
sidebar_position: 7
---

# Microclimates



## References

[???]---
sidebar_position: 6
---

# Least cost path wildlife corridors
Identify the current locations of important wildlife habitats in the study area, such as parks, green spaces, and natural areas.
Identify the current barriers to wildlife movement, such as roads, railways, and other infrastructure.
Identify potential corridors that could be used by wildlife to move between habitats, such as the edges of railways and roads, the ends of public gardens, and parts of parks.
Evaluate the feasibility of using each potential corridor for wildlife movement, taking into account factors such as the potential impact on human uses of the area, the potential for conflicts with other land uses, and the potential for habitat degradation or loss.
Use GIS software to create a map showing the proposed improvements to the urban wildlife corridors, including any modifications that may be needed to make the corridors more usable by wildlife (e.g. the installation of wildlife crossings or fencing).
Consult with relevant stakeholders, such as local governments, community groups, and environmental organizations, to solicit feedback on the proposed improvements and incorporate any necessary revisions.


### Show column names
``` sql
SELECT COLUMN_NAME
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_NAME = 'topographicarea'
```

``` sql
SELECT *
FROM topographicarea
LIMIT 1000
```
 
### Show entries in descriptivegroup column
``` sql
SELECT descriptivegroup
FROM topographicarea
```

### Show entries in descriptivegroup column
``` sql
SELECT descriptiveterm
FROM topographicarea
```

### Select scrub land
``` sql
SELECT fid, descriptiveterm, wkb_geometry
FROM topographicarea
WHERE 'Natural Environment' = ANY (descriptivegroup)
```

``` sql
SELECT fid, descriptiveterm, wkb_geometry
FROM topographicarea
WHERE '%Rail%' = LIKE (descriptiveterm)
```

![Docusaurus Plushie](/img/wildlife-corridors-4.png)

wildlife-corridors-5.png

## References
---
sidebar_position: 3
---

# Flood risk

To calculate flood risk from LiDAR data in PostGIS, you will need to follow these steps:

Import the LiDAR data into PostGIS. This can be done using the ST_LASToSQL function, which will convert the LiDAR data into a format that can be stored in a PostGIS database.

Use the ST_Union function to merge all of the LiDAR points into a single geometry. This will create a "point cloud" representation of the terrain.

Use the ST_Triangulate function to triangulate the point cloud. This will create a set of triangular facets, each of which represents a portion of the terrain surface.

Use the ST_Dump function to extract the individual triangles from the triangulated surface.

Use the ST_Z function to extract the elevation of each vertex of each triangle.

Use the ST_Area function to calculate the area of each triangle.

Use the ST_Centroid function to calculate the centroid of each triangle.

Use the ST_Distance function to calculate the distance from the centroid of each triangle to the nearest river or stream.

Use the elevation and distance information to calculate the flood risk for each triangle.

Use the ST_Union function to merge all of the triangles into a single polygon, and use the ST_ConvexHull function to create a convex hull around the polygon.

Use the ST_Intersection function to calculate the intersection between the convex hull and the floodplain.

Use the ST_Area function to calculate the area of the intersection, and use this value to calculate the overall flood risk for the region.


## References---
sidebar_position: 3
---

# Land registry price paid parcels
We combined three separate files containing price paid data into a single file, removed unnecessary quotes, selected only rows that contained the string "GL" followed by a number between 0 and 9, and printed out only the fourth and second columns. We also added column names and deleted any rows that contained null values.
```
$ cat pp-2018.csv pp-2019.csv pp-2020.csv | tr -d '"' > pp_3year.csv \
&& awk -F"," '/GL+[0-9]/ { print $4 "," $2}' pp_3year.csv > gl_p_3.csv \
&& { echo "postcode, pounds"; cat gl_p_3.csv; } > prices.csv \
&& sed -i '/\\N/d' prices.csv
```
## Prepare location data

We used the same process to clean and filter the data as we did for the price data, except we did not need to concatenate multiple files together.

```
$ awk -F"," '/GL+[0-9]/ { print $1 "," $8 "," $9}' open_postcode_geo.csv >
gl_l.csv \
&& { echo "postcode, latitude, longitude"; cat gl_l.csv; } >
coordinates.csv \
&& sed -i '/\\N/d' coordinates.csv
```
## Import parcels


We used ogr2ogr to convert a file containing cadastral parcel information (in GML format) into a PostgreSQL file, changed the projection of the data from OSGB to WGS84, and imported it into a database.

```
ogr2ogr \
-f "PostgreSQL" \
-a_srs "EPSG:27700" \
-t_srs "EPSG:27700" \
-nln parcels \
-progress \
PG:"dbname='postgres' host='0.0.0.0' port='5432' user='postgres'
password='postgres'" \
Land_Registry_Cadastral_Parcels.gml
```
## Connect to server

## set psql password
``` bash
export PGPASSWORD=postgres
```

We started a psql session on the client computer to allow us to communicate with the database that is stored on the server.

``` bash
psql -h 0.0.0.0 -U postgres
```
## Create prices table

We created a new empty table with a primary key column of type serial (which will automatically increment) and two additional columns: one for text data (postcodes) and one for integer data (pounds).

```
CREATE TABLE prices (
p_prices_id serial PRIMARY KEY,
p_postcode TEXT NOT NULL,
pounds INTEGER NOT NULL
);
```
## Create location table

We created a similar empty table for storing location data, but with columns for latitude and longitude rather than a column for pounds.

```
CREATE TABLE coordinates (
c_id serial PRIMARY KEY,
c_postcode TEXT NOT NULL,
latitude FLOAT NOT NULL,
longitude FLOAT NOT NULL
);
```
## Populate prices table


We used the \copy command in psql to import the price data into the new price column in the database.

```
\copy prices(p_postcode, pounds) FROM '/home/reuben/Downloads/prices.csv'
DELIMITER ',' CSV HEADER;
```
## Populate coordinates table

We repeat the process for the coordinates data.

```
\copy coordinates(c_postcode, latitude, longitude) FROM
'/home/reuben/Downloads/coordinates.csv' DELIMITER ',' CSV HEADER;
```
# Join coordinates and prices into points

We used the JOIN command in SQL to create a new table that combines the prices and coordinates data based on their shared postcodes.

### SELECT

```
c_id,
c_postcode,
latitude,
longitude,
pounds
INTO points
FROM coordinates INNER JOIN prices
ON coordinates.c_postcode = prices.p_postcode;
```
# Add geometry column to points

We added a new column to the table to store geometry data.

```
ALTER TABLE points ADD COLUMN geom GEOMETRY(Point, 4326 );
```
# Update points from coordinates

We used the data in the latitude and longitude columns to create points and stored them in the geometry column.


```
UPDATE points SET geom = ST_SETSRID(ST_MakePoint(longitude,
latitude), 4326 );
```
# Create priced polygons

For each point within a polygon, we created a new polygon and added the corresponding price paid for the point to it.
### SELECT

```
c_id,
parcels.wkb_geometry,
points.pounds
INTO polygons
FROM
parcels INNER JOIN points
ON st_contains(parcels.wkb_geometry, points.geom);
```
## Find avarage point value for duplicate polygons

We calculated the average value for each of the duplicate polygons.

```
SELECT c_id,geom,avg(pounds)
INTO avg_polygons
FROM polygons
GROUP BY geom;
```
## Import new price paid polygons to file

We used QGIS to export the table from the database.

```
Qgis > Database > DB Manager > Import Layer/File - Name: avg_polygons
```
## Add price paid polygons layer to Qgis

Displaying the layer in Qgis.

```
Qgis > Layer > Add Layer > Add Vector Layer
Qgis > Database > DB Manager > Import Layer/File - Name: avg_polygons
Vector Dataset(s): .shp
```
## Colour polygons by attribute field

We modified the layer properties in order to create a visually appealing effect.

```
Right click: Layer > Properties
```

```
Symbology > Single Symbol: Graduated
Value: pounds
Colour Ramp: Spectral
Invert Colour Ramp
Segmentation: Equal Interval
```
![Docusaurus Plushie](/img/price-paid-parcel.jpg)




















## Download data

We downloaded a CSV file that contained property price data for the past three years, along with the postal code coordinates and boundary line polygons.

```
$ wget bdline_gpkg_gb.zip
&& unzip bdline_gpkg_gb.zip
&& cd data
```
## Import bdline

We used ogr2ogr to convert a file containing boundary lines (in a format called GeoPackage) into a PostgreSQL file, changed the projection of the data from OSGB1936 to WGS84, and imported it into a database.

```
ogr2ogr \
-f "PostgreSQL" \
-a_srs "EPSG:27700" \
```

```
-t_srs "EPSG:4326" \
-progress PG:"dbname='gis' host='$ip' port='5432' user='$user'
password='$password'" \
bdline_gb.gpkg
```
## Connect to server

Starting a psql instance on the client in order to interact with the database on the server.

```
psql -h 192.168.88.10 -U postgres gis
```
# Create priced paid polygons for every point

We used point data that was already in the database from a previous project to create a new polygon for each point that was within the boundaries of a parish. We also added the price paid for each house (the point) to the corresponding polygon.

### SELECT

```
parish.geom,
points.pounds
INTO pp_parish
FROM
parish INNER JOIN points
ON st_contains(parish.geom, points.geom);
```
## Find avarage point value for duplicate polygons

Like in the the previous project, we calculated the average value for each of the duplicate polygons.

```
SELECT geom,avg(pounds)
INTO avg_pp_parish
FROM pp_parish
GROUP BY geom;
```
## Import new price paid polygons to file

```
Qgis > Database > DB Manager > Import Layer/File - Name: pp_parish
```
## Add price paid polygons layer to Qgis

```
Qgis > Layer > Add Layer > Add Vector Layer
Qgis > Database > DB Manager > Import Layer/File - Name: pp_parish
Vector Dataset(s): .shp
```

## Colour polygons by attribute field

```
Right click: Layer > Properties
Symbology > Single Symbol: Gradiated
Vaule: pounds
Colour Ramp: Spectral
Invert Colour Ramp
Segmentation: Equal Interval
```---
sidebar_position: 1
---

# Setting up PostGIS Database Server

### Create a pod
``` bash
podman pod create -p 8080:8080 -p 5432:5432 -n geospatial
```

### Run image in pod
``` bash
podman run \
--pod geospatial \
--name postgis \
-e POSTGRES_PASSWORD=postgres \
-d postgis/postgis
```

### auto gen systemd file
``` bash
podman generate systemd postgis >
/home/$user/.config/systemd/user/postgis.service
```

### start postgis service
``` bash
systemctl start --user postgis.service
```

### enable postgis service
``` bash
systemctl start --user postgis.service
```

``` bash
podman start postgis
```
## Check
``` bash
psql -h localhost -p 5432 -U postgres 
```


### Generate YAML file
``` bash
podman generate kube geospatial -f geospatial-stack-podman.yaml
```

## Here's the YAML file
``` yaml

# Save the output of this file and use kubectl create -f to import
# it into Kubernetes.
#
# Created with podman-4.3.1
apiVersion: v1
kind: Pod
metadata:
  annotations:
    io.kubernetes.cri-o.ContainerType/geoserver: container
    io.kubernetes.cri-o.ContainerType/postgis: container
    io.kubernetes.cri-o.SandboxID/geoserver: 5579c50f78551192cbc7eafed6f04db71155c3690d677c9a5741b25fe54fc14
    io.kubernetes.cri-o.SandboxID/postgis: 5579c50f78551192cbc7eafed6f04db71155c3690d677c9a5741b25fe54fc14
    io.kubernetes.cri-o.TTY/geoserver: "true"
    io.kubernetes.cri-o.TTY/postgis: "true"
    io.podman.annotations.autoremove/geoserver: "FALSE"
    io.podman.annotations.autoremove/postgis: "FALSE"
    io.podman.annotations.init/geoserver: "FALSE"
    io.podman.annotations.init/postgis: "FALSE"
    io.podman.annotations.privileged/geoserver: "FALSE"
    io.podman.annotations.privileged/postgis: "FALSE"
    io.podman.annotations.publish-all/geoserver: "FALSE"
    io.podman.annotations.publish-all/postgis: "FALSE"
  creationTimestamp: "2022-12-30T13:47:48Z"
  labels:
    app: geospatial
  name: geospatial
spec:
  automountServiceAccountToken: false
  containers:
  - env:
    - name: GEOSERVER_ADMIN_USER
      value: postgres
    - name: GEOSERVER_ADMIN_PASSWORD
      value: postgres
    image: docker.io/kartoza/geoserver:latest
    name: geoserver
    ports:
    - containerPort: 5432
      hostPort: 5432
    - containerPort: 8080
      hostPort: 8080
    resources: {}
    securityContext:
      capabilities:
        drop:
        - CAP_MKNOD
        - CAP_NET_RAW
        - CAP_AUDIT_WRITE
    tty: true
  - env:
    - name: POSTGRES_PASS
      value: postgres
    - name: POSTGRES_USER
      value: postgres
    image: docker.io/kartoza/postgis:latest
    name: postgis
    resources: {}
    securityContext:
      capabilities:
        drop:
        - CAP_MKNOD
        - CAP_NET_RAW
        - CAP_AUDIT_WRITE
    tty: true
    volumeMounts:
    - mountPath: /var/lib/postgresql
      name: 3276e85c3fc649e1904e0161ff79599dcc3292e9121982d648c0f2c47585abaf-pvc
  enableServiceLinks: false
  hostname: geospatial
  restartPolicy: Never
  volumes:
  - name: 3276e85c3fc649e1904e0161ff79599dcc3292e9121982d648c0f2c47585abaf-pvc
    persistentVolumeClaim:
      claimName: 3276e85c3fc649e1904e0161ff79599dcc3292e9121982d648c0f2c47585abaf
status: {}
```

### Test YAML file

#### Remove postgis container
``` bash
podman rm -vf postgis
```

#### Remove geoserver container
``` bash
podman rm -vf geoserver
```

#### Remove spatial pod
``` bash
podman pod rm spatial
```

#### Install everything again using the YAML file
``` bash
podman play kube geospatial-stack-podman.yaml
```---
sidebar_position: 2
---

# Network Status Map

## To do
- cross platform smart phone app that can scan qr code and post to database along with gps location
- can select whether being deployed or derigged after scanned
- show map of network status of devices on map
- database connects to cloud controller / router for network status




## References
---
sidebar_position: 2
---

# Toggle map

``` php
<?php

// Connect to the database
$db = new PDO('pgsql:host=your_host;dbname=your_database', 'your_username', 'your_password');

// Get the geometry data from the database
$stmt = $db->prepare('SELECT * FROM your_table WHERE your_column = :value');
$stmt->execute(array(':value' => $_GET['value']));
$geometry = $stmt->fetchAll(PDO::FETCH_ASSOC);

// Return the geometry data as JSON
header('Content-Type: application/json');
echo json_encode($geometry);

```

``` javascript
<!-- Add the Leaflet CSS and JavaScript files to the page -->
<link rel="stylesheet" href="https://unpkg.com/leaflet@1.7.1/dist/leaflet.css" />
<script src="https://unpkg.com/leaflet@1.7.1/dist/leaflet.js"></script>

<!-- Create a div element for the map -->
<div id="map"></div>

<!-- Add buttons to the page -->
<button id="button1">Button 1</button>
<button id="button2">Button 2</button>

<!-- Initialize the map and add a tile layer -->
<script>
  // Create the map and set the view to a default location
  var map = L.map('map').setView([51.505, -0.09], 13);

  // Add a tile layer to the map
  L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
    attribution: 'Map data &copy; <a href="https://www.openstreetmap.org/">OpenStreetMap
```---
sidebar_position: 2
---

# Loading Land registry inspire polygons

### Download data

Downloading a csv file of property price paid data for each of the last three years, postcode coordinates, and land registry cadastral parcels for Stoud.

``` bash
wget https://use-land-property-
data.service.gov.uk/datasets/inspire/download/Stroud.zip \
&& unzip Stroud.zip
```

``` bash
psql -h localhost -p 5432 -U postgres 
```

### create table
```sql
CREATE TABLE inspire-polygons-stroud (
  geom geometry(Geometry,4326),
  properties jsonb
);
```

### Import parcels
Using ogr2ogr 
``` bash
ogr2ogr -f "PostgreSQL" "PG:host=0.0.0.0 user=postgres dbname=public password=postgres" *.shp -nln inspire-polygons-stroud
```

## Create spatial index
``` sql
CREATE INDEX inspire-polygons-stroud-gist ON inspire-polygons-stroud USING GIST (geom);
```

## References
---
sidebar_position: 4
---

# Loading Ordnance Survey MasterMap with astun loader


## To do
- Setup loader in pod?

### Install astun loader

Download loader

``` bash
git clone https://github.com/AstunTechnology/Loader.git

```

### Change directory

``` bash
cd Loader
```

Install dependancies 

``` bash
sudo dnf install gdal
```

make directories

``` bash
mkdir source temporary output
```

### Prepare data

Download MasterMap data

wget ???

Unzip mastermap data to source directory

``` bash
unzip OS_order_6148593_OSMasterMapTopography5km_2022-12-22_1.zip -d source
```

Remove manifest text file in source directory

``` bash
rm source/manifest.txt
```

### Edit loader configuration

Backup original

``` bash
cp python/loader.config python/loader.config.bak
```

Replace line 8 source directory

``` bash
sed -i '8s/.*/src_dir=\$HOME\/Loader\/source/' python/loader.config
```

Replace line 13 temp directory

``` bash
sed -i '13s/.*/tmp_dir=\$HOME\/Loader\/temporary/' python/loader.config
```

Replace line 17 output directory

``` bash
sed -i '17s/.*/out_dir=\$HOME\/Loader\/output/' python/loader.config
```

Change line 29 database parameters

``` bash
sed -i '29s/.*/ogr_cmd=ogr2ogr --config GML_EXPOSE_FID NO -append -skipfailures -f PostgreSQL PG:\x27dbname=postgres active_schema=public host=0\.0\.0\.0 user=postgres password=postgres\x27 \$file_path/' python/loader.config
```

### Run ashtun loader

Change directory

``` bash
cd python
```





Run

``` bash
python loader.py loader.config
```

### Qgis style

Launch qgis

> super, then type qgis, then enter

Create new project

Set default CRS to OSGB

Connect to PostGIS

connect to postgres

Add each layer to project

Download styles

``` bash
git clone https://github.com/OrdnanceSurvey/OS-Master-Map-Topography.git
```

Attach corrosponding styles to each layer

Copy symbols

``` bash
cp -R osmmsymbol $directory $svgdirectory
```

Copy font

``` bash
cp font to font directory
```

Make Qgis plugins work



## References
---
sidebar_position: 5
---

# Ordnance survey satellite imagery with raster2pgsql

### Load sattelite data with either raster2pgsql or ogr2ogr

### Connect to database

``` bash
psql -h 0.0.0.0 -p 5432 -U postgres  -d postgres
```

``` sql
CREATE EXTENSION postgis_raster;
```

### Load sattelite data with either raster2pgsql or ogr2ogr


### unzip all zips
``` bash
find . -name "*.zip" -exec unzip {} \;
```

## set psql password
``` bash
export PGPASSWORD=postgres
```


### raster2pgsql
``` bash
raster2pgsql -s 27700 -I -C -M -F  *.JPG public.bristol_osmm_satellite_imagery | psql -h 0.0.0.0 -p 5432 -U postgres 
```


### Add a spatial index

``` sql
CREATE INDEX bristol_osmm_satellite_imagery__gist ON bristol_osmm_satellite_imagery_ USING GIST (ST_ConvexHull(rast));
```

## References

---
sidebar_position: 3
---

# Ordnance survey lidar data with raster2pgsql

### Load lidar data with either raster2pgsql 

### Connect to database

``` bash
psql -h 0.0.0.0 -p 5432 -U postgres  -d postgres
```

``` sql
CREATE EXTENSION postgis_raster;
```

### Load sattelite data with either raster2pgsql or ogr2ogr

### unzip all zips
``` bash
find . -name "*.zip" -exec unzip {} \;
```

## set psql password
``` bash
export PGPASSWORD=postgres
```

### raster2pgsql
``` bash
raster2pgsql -s 27700 -I -C -M -F  *.tif public.bristol_lidar_dtm_1m | psql -h 0.0.0.0 -p 5432 -U postgres --password postgres
```

### Add a spatial index

``` sql
CREATE INDEX bristol_lidar_dtm_gist ON bristol_lidar_dtm_1m USING GIST (ST_ConvexHull(rast));
```

## References

---
sidebar_position: 1
---

# Loading Bristol city council GeoJson files

### Loop over geojson files and create a new table for each and upload

This script will create a new table in the PostgreSQL database for each .geojson file in the specified directory, using the file name (without the .geojson extension) as the table name. The GeoJSON data from each file will be imported into the corresponding table.
```bash
#!/bin/bash

# Set the database connection string
conn_string="host=0.0.0.0 user=postgres dbname=postgres password=postgres"

# Loop over all .geojson files in the directory
for file in *.geojson; do
    # Extract the file name without the .geojson extension
    tablename=$(basename "$file" .geojson)
    # Use ogr2ogr to import the file into a new table
    ogr2ogr -f "PostgreSQL" -s_srs EPSG:4326 -t_srs EPSG:27700 PG:"$conn_string" "$file" -nln "$tablename"
done
```


### Add a spatial index

``` sql
CREATE INDEX bristol-council ON bristol-council USING GIST (geom);
```


## References


---
sidebar_position: 1
---

# Identify flat terrain with above average sun

To identify flat areas in lidar data using GDAL and GRASS in Python, you can use the following steps:

### Import the necessary modules

``` python
from osgeo import gdal
import grass.script as gs
```

### Set the GRASS GIS environment

``` python
gisbase = '/usr/local/grass78'
gs.set_gisbase(gisbase)
location = 'location'
mapset = 'mapset'
gs.run_command('g.proj', georef='path/to/georeferenced_file.tif', location=location)
```

### Import the lidar data into GRASS GIS

``` python
gs.run_command('r.in.lidar', input='path/to/lidar_data.las', output='lidar_data', flags='e')
```

Calculate the slope of the lidar data using the r.slope.aspect module:
``` python
gs.run_command('r.slope.aspect', elevation='lidar_data', slope='slope', aspect='aspect')
``` 

### Identify flat areas by selecting pixels with a slope less than a certain threshold

``` python
gs.mapcalc("flat = if(slope < 0.1, 1, null())")
```

If you want to also consider sunlight exposure, you can use the r.sun module to calculate solar radiation and sky view factor. For example:

``` python
gs.run_command('r.sun', elevation='lidar_data', solar_radiation='solar_radiation', sky_view_factor='sky_view_factor')
```

You can then use map algebra to select pixels that have both low slope and high solar radiation or sky view factor. For example:
``` python
gs.mapcalc("flat_sunny = if(slope < 0.1 && solar_radiation > 500, 1, null())")
```